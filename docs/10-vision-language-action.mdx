---
id: 10-vision-language-action
title: "Chapter 10: Vision-Language-Action (VLA) Models"
sidebar_position: 10
part: 4
week: 13
difficulty_levels: [advanced]
hardware_tracks: [simulation_only, budget_hardware, research_grade]
citation_count: 20
word_count: 8500
urdu_completeness: 0
---

# Chapter 10: Vision-Language-Action (VLA) Models

> *"Language is the scaffold that connects perception to action."*

## Learning Objectives

By the end of this chapter, you will be able to:

1. **Understand Vision-Language-Action** model architecture
2. **Implement voice interfaces** with OpenAI Whisper
3. **Use LLMs to plan robot actions** (natural language → ROS)
4. **Deploy RT-2 / OpenVLA** for robotic control
5. **Build end-to-end voice-controlled** robot

**Estimated Time**: 10-12 hours (reading + labs)

---

## 10.1 Introduction: From Chatbots to Robots

**Large Language Models (LLMs)** like GPT-4 and Claude excel at reasoning about text. But can they control robots?

**Challenge**: LLMs operate on tokens (words). Robots need **actions** (joint torques, velocities).

**Vision-Language-Action (VLA) Models** bridge this gap:
- **Input**: Camera image + text instruction ("pick up the red block")
- **Output**: Robot action (7D vector: x, y, z, roll, pitch, yaw, gripper)

### 10.1.1 Evolution

**2022**: RT-1 (Google Robotics)
- 13M parameters
- Trained on 130k robot demonstrations
- Success rate: 70% on seen tasks

**2023**: RT-2 (Google DeepMind)
- 55B parameters (uses PaLM-E vision-language model)
- Trained on web images + robot data
- **Key insight**: Web knowledge transfers to robotics (knows "apple" is edible even if never trained to grasp apples)
- Success rate: 83% on seen tasks, 62% on novel tasks

**2024**: OpenVLA (UC Berkeley + Stanford)
- 7B parameters
- **Open-source** (unlike RT-1/RT-2)
- Trained on Open X-Embodiment dataset (1M+ trajectories, 22 robots)
- Runs on NVIDIA Jetson Orin (edge deployment)

---

## 10.2 VLA Architecture

### 10.2.1 Components

```
┌─────────────────────────────────────────────────────────┐
│                     VLA Model (RT-2)                    │
│                                                         │
│  ┌──────────────┐     ┌──────────────┐                │
│  │   Vision     │     │   Language   │                │
│  │   Encoder    │     │   Encoder    │                │
│  │   (ViT)      │     │   (T5)       │                │
│  └──────┬───────┘     └──────┬───────┘                │
│         │                     │                         │
│         └──────────┬──────────┘                         │
│                    ▼                                    │
│         ┌──────────────────────┐                        │
│         │   Cross-Modal        │                        │
│         │   Fusion Layer       │                        │
│         └──────────┬───────────┘                        │
│                    ▼                                    │
│         ┌──────────────────────┐                        │
│         │   Action Decoder     │                        │
│         │   (Transformer)      │                        │
│         └──────────┬───────────┘                        │
│                    ▼                                    │
│              [x, y, z, R, P, Y, gripper]                │
└─────────────────────────────────────────────────────────┘
```

### 10.2.2 Vision Encoder (ViT)

**Vision Transformer (ViT)** processes camera images:

1. **Patch embedding**: Split image into 16×16 patches (224×224 image → 196 patches)
2. **Positional encoding**: Add position information
3. **Transformer layers**: Self-attention + feedforward (12-24 layers)
4. **Output**: 196 patch tokens (each 768D)

**Code Example**: ViT in PyTorch

```python
import torch
from transformers import ViTModel

# Load pre-trained ViT
model = ViTModel.from_pretrained('google/vit-base-patch16-224')

# Input: RGB image (224x224)
image = torch.randn(1, 3, 224, 224)

# Forward pass
outputs = model(pixel_values=image)

# Last hidden state: (1, 197, 768)
#   197 = 196 patches + 1 [CLS] token
#   768 = embedding dimension
features = outputs.last_hidden_state

print(features.shape)
# Output: torch.Size([1, 197, 768])
```

### 10.2.3 Language Encoder (T5)

**T5 (Text-to-Text Transfer Transformer)** processes instructions:

**Input**: "pick up the red block"
**Output**: Contextualized token embeddings

```python
from transformers import T5Tokenizer, T5EncoderModel

# Load T5 encoder
tokenizer = T5Tokenizer.from_pretrained('t5-base')
model = T5EncoderModel.from_pretrained('t5-base')

# Tokenize instruction
text = "pick up the red block"
inputs = tokenizer(text, return_tensors='pt')

# Encode
outputs = model(**inputs)

# Last hidden state: (1, seq_len, 768)
language_features = outputs.last_hidden_state

print(language_features.shape)
# Output: torch.Size([1, 6, 768])  (6 tokens)
```

### 10.2.4 Cross-Modal Fusion

**Combine vision and language** using cross-attention:

```python
import torch.nn as nn

class CrossModalFusion(nn.Module):
    def __init__(self, dim=768):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, num_heads=8)

    def forward(self, vision_features, language_features):
        """
        Args:
            vision_features: (seq_len_v, batch, dim)
            language_features: (seq_len_l, batch, dim)

        Returns:
            fused_features: (seq_len_v, batch, dim)
        """
        # Cross-attention: language attends to vision
        fused, _ = self.cross_attn(
            query=language_features,
            key=vision_features,
            value=vision_features
        )
        return fused
```

### 10.2.5 Action Decoder

**Transformer decoder** predicts action autoregressively:

**Output**: 7D action vector
- `(x, y, z)`: End-effector position (meters)
- `(roll, pitch, yaw)`: End-effector orientation (radians)
- `gripper`: 0 = open, 1 = closed

**Action discretization** (RT-2 approach):
- Bin continuous actions into 256 discrete values
- Example: x ∈ [-0.5, 0.5] → bin into 256 levels
- **Why?** Transformers predict discrete tokens (easier to train)

```python
def discretize_action(action, bins=256, action_range=(-0.5, 0.5)):
    """
    Discretize continuous action into bins.

    Args:
        action: float in action_range
        bins: number of discrete levels
        action_range: (min, max)

    Returns:
        token: integer in [0, bins-1]
    """
    min_val, max_val = action_range
    normalized = (action - min_val) / (max_val - min_val)  # [0, 1]
    token = int(normalized * (bins - 1))
    return np.clip(token, 0, bins - 1)

# Example
x_action = 0.25  # meters
token = discretize_action(x_action, bins=256, action_range=(-0.5, 0.5))
print(f"Continuous: {x_action} → Discrete: {token}")
# Output: Continuous: 0.25 → Discrete: 191
```

---

## 10.3 Voice-to-Text (OpenAI Whisper)

**Whisper** is an automatic speech recognition (ASR) model trained on 680,000 hours of multilingual data.

### 10.3.1 Whisper Architecture

- **Encoder**: Audio spectrogram → hidden states
- **Decoder**: Transformer generates text tokens
- **Models**: tiny (39M), base (74M), small (244M), medium (769M), large (1550M)

**Performance**:
- **Word Error Rate (WER)**: 5-10% (comparable to human transcription)
- **Latency**: 0.5-2 seconds (depending on model size)

### 10.3.2 Installation

```bash
pip install openai-whisper
```

### 10.3.3 Basic Usage

```python
import whisper

# Load model
model = whisper.load_model("base")  # Options: tiny, base, small, medium, large

# Transcribe audio file
result = model.transcribe("audio.mp3")

print(result["text"])
# Output: "Robot, pick up the red block and place it on the table."
```

### 10.3.4 Real-Time Voice Input

**Capture microphone audio** with PyAudio:

```python
import pyaudio
import wave
import whisper

def record_audio(filename="command.wav", duration=5):
    """
    Record audio from microphone.

    Args:
        filename: Output WAV file
        duration: Recording duration (seconds)
    """
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000  # Whisper expects 16kHz

    p = pyaudio.PyAudio()

    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)

    print("Recording...")
    frames = []

    for _ in range(0, int(RATE / CHUNK * duration)):
        data = stream.read(CHUNK)
        frames.append(data)

    print("Done recording.")

    stream.stop_stream()
    stream.close()
    p.terminate()

    # Save to file
    wf = wave.open(filename, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

# Record and transcribe
record_audio("command.wav", duration=5)
model = whisper.load_model("base")
result = model.transcribe("command.wav")
print(f"You said: {result['text']}")
```

### 10.3.5 ROS Integration

**Create ROS 2 node** for voice commands:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command')
        self.publisher_ = self.create_publisher(String, 'voice_commands', 10)
        self.model = whisper.load_model("base")

        # Record and publish every 5 seconds
        self.timer = self.create_timer(5.0, self.listen_callback)

    def listen_callback(self):
        # Record audio
        record_audio("temp.wav", duration=3)

        # Transcribe
        result = self.model.transcribe("temp.wav")
        text = result["text"]

        # Publish
        msg = String()
        msg.data = text
        self.publisher_.publish(msg)
        self.get_logger().info(f'Published command: "{text}"')

def main():
    rclpy.init()
    node = VoiceCommandNode()
    rclpy.spin(node)
```

---

## 10.4 Language-to-Action Planning (GPT-4)

**Use GPT-4 to decompose** natural language commands into robot actions.

### 10.4.1 Prompt Engineering

**System prompt** defines robot capabilities:

```python
SYSTEM_PROMPT = """
You are a robot controller. You have access to the following actions:

1. navigate_to(location: str) - Move to a named location (e.g., "kitchen", "table")
2. grasp_object(object: str) - Pick up an object by name (e.g., "cup", "book")
3. place_object(location: str) - Put down the currently held object
4. open_gripper() - Open the gripper
5. close_gripper() - Close the gripper

Given a user command, output a JSON list of actions to execute.

Example:
User: "Bring me a water bottle from the kitchen"
Output:
[
  {"action": "navigate_to", "location": "kitchen"},
  {"action": "grasp_object", "object": "water bottle"},
  {"action": "navigate_to", "location": "user"}
]
"""
```

### 10.4.2 GPT-4 API Call

```python
import openai
import json

openai.api_key = "YOUR_API_KEY"

def plan_actions(user_command):
    """
    Use GPT-4 to plan action sequence.

    Args:
        user_command: Natural language instruction

    Returns:
        List of action dictionaries
    """
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_command}
        ],
        temperature=0.0  # Deterministic output
    )

    # Parse JSON response
    actions_json = response.choices[0].message.content
    actions = json.loads(actions_json)

    return actions

# Example
command = "Robot, clean the table by moving all objects to the bin"
actions = plan_actions(command)

for action in actions:
    print(action)

# Output:
# {'action': 'navigate_to', 'location': 'table'}
# {'action': 'grasp_object', 'object': 'object_1'}
# {'action': 'navigate_to', 'location': 'bin'}
# {'action': 'place_object'}
# {'action': 'navigate_to', 'location': 'table'}
# ...
```

### 10.4.3 Executing Actions with ROS 2

**Action executor** translates GPT-4 output to ROS actions:

```python
import rclpy
from rclpy.action import ActionClient
from nav2_msgs.action import NavigateToPose
from moveit_msgs.action import MoveGroup

class ActionExecutor(Node):
    def __init__(self):
        super().__init__('action_executor')

        # Action clients
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')
        self.moveit_client = ActionClient(self, MoveGroup, 'move_action')

    def execute_action(self, action):
        """
        Execute a single action.

        Args:
            action: Dict with 'action' field and parameters
        """
        action_type = action['action']

        if action_type == 'navigate_to':
            self.navigate(action['location'])
        elif action_type == 'grasp_object':
            self.grasp(action['object'])
        elif action_type == 'place_object':
            self.place(action.get('location'))
        else:
            self.get_logger().error(f"Unknown action: {action_type}")

    def navigate(self, location):
        """Navigate to named location."""
        # Map location names to coordinates
        locations = {
            'kitchen': (2.0, 1.0, 0.0),
            'table': (1.0, 0.0, 0.0),
            'user': (0.0, 0.0, 0.0),
        }

        if location not in locations:
            self.get_logger().error(f"Unknown location: {location}")
            return

        x, y, theta = locations[location]

        # Create navigation goal
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = x
        goal_msg.pose.pose.position.y = y
        goal_msg.pose.pose.orientation.w = 1.0

        # Send goal
        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal_msg)
        rclpy.spin_until_future_complete(self, future)

        self.get_logger().info(f"Navigated to {location}")

    def grasp(self, object_name):
        """Grasp object by name."""
        # 1. Detect object (vision)
        object_pose = self.detect_object(object_name)

        # 2. Plan grasp with MoveIt
        # (Similar to Chapter 9 examples)
        self.get_logger().info(f"Grasped {object_name}")

    def place(self, location):
        """Place currently held object."""
        self.get_logger().info(f"Placed object at {location}")
```

---

## 10.5 RT-2: Vision-Language-Action Model

**RT-2** (Robotics Transformer 2) is Google DeepMind's state-of-the-art VLA model.

### 10.5.1 Training Data

**Web images** (billions):
- COCO, ImageNet, CC3M
- Paired with captions ("a person holding a red apple")

**Robot demonstrations** (130k):
- 13 different robots
- 700+ tasks (pick, place, push, open, close)
- Recorded in real kitchens, offices, labs

**Key insight**: Pre-training on web data provides **world knowledge**.

### 10.5.2 Zero-Shot Transfer

**Example**:
- **Training**: Only trained to grasp "soup cans"
- **Test**: Asked to grasp "ketchup"
- **Result**: Success! (knows ketchup is similar to soup can)

**Another example**:
- **Training**: Never trained on "endangered animals"
- **Test**: "Move the toy to the endangered animal"
- **Result**: Moves toy to panda plushie (knows pandas are endangered)

### 10.5.3 Running RT-2 (Requires Google Cloud TPU)

**Note**: RT-2 is not open-source. This is a conceptual example.

```python
# Hypothetical API
from rt2 import RT2Model

# Load pre-trained model (requires TPU)
model = RT2Model.load("rt2-55b")

# Inference
image = capture_camera_image()  # 224x224 RGB
instruction = "pick up the apple"

action = model.predict(image, instruction)

print(action)
# Output: {
#   'position': [0.45, 0.12, 0.35],
#   'orientation': [0.0, 0.0, 1.57],
#   'gripper': 1.0  # closed
# }

# Execute action
robot.move_to(action['position'], action['orientation'])
robot.set_gripper(action['gripper'])
```

---

## 10.6 OpenVLA (Open-Source Alternative)

**OpenVLA** is an open-source VLA model (7B parameters).

### 10.6.1 Installation

```bash
# Clone repository
git clone https://github.com/openvla/openvla.git
cd openvla

# Install dependencies
pip install -r requirements.txt

# Download pre-trained checkpoint (7GB)
wget https://huggingface.co/openvla/openvla-7b/resolve/main/openvla-7b.pth
```

### 10.6.2 Inference

```python
import torch
from openvla import OpenVLA
from PIL import Image

# Load model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = OpenVLA.from_pretrained("openvla-7b").to(device)

# Capture image
image = Image.open("camera_view.jpg")

# Instruction
instruction = "pick up the red cup"

# Predict action
with torch.no_grad():
    action = model.predict(image, instruction)

print(f"Position: {action['position']}")
print(f"Orientation: {action['orientation']}")
print(f"Gripper: {action['gripper']}")

# Output:
# Position: [0.452, 0.123, 0.347]
# Orientation: [0.0, 0.0, 1.571]
# Gripper: 0.98
```

### 10.6.3 Fine-Tuning on Custom Tasks

**Collect demonstrations**:

```python
# Record demonstrations
demonstrations = []

for episode in range(100):
    # Human teleoperates robot
    obs = []
    actions = []

    for step in range(50):
        image = capture_camera()
        action = teleop_device.read()  # Joystick, VR controller

        obs.append(image)
        actions.append(action)

    demonstrations.append({
        'instruction': "pick and place",
        'observations': obs,
        'actions': actions
    })

# Save dataset
torch.save(demonstrations, "custom_dataset.pt")
```

**Fine-tune**:

```bash
python train.py \
  --model openvla-7b \
  --dataset custom_dataset.pt \
  --epochs 10 \
  --lr 1e-5 \
  --output fine_tuned_model.pth
```

### 10.6.4 Deployment on Jetson Orin

**Optimize for edge inference**:

```bash
# Convert to TensorRT
python export_tensorrt.py \
  --checkpoint fine_tuned_model.pth \
  --output openvla_trt.engine \
  --fp16  # Half-precision (2x faster)
```

**Run on Jetson**:

```python
import tensorrt as trt

# Load TensorRT engine
with open("openvla_trt.engine", "rb") as f:
    engine = trt.Runtime(trt.Logger()).deserialize_cuda_engine(f.read())

context = engine.create_execution_context()

# Inference loop
while True:
    image = capture_camera()
    instruction = get_latest_command()  # From voice or text

    # Run inference
    action = run_trt_inference(context, image, instruction)

    # Execute
    robot.execute_action(action)

    time.sleep(0.1)  # 10 Hz
```

---

## 10.7 Multi-Modal Interaction

### 10.7.1 Speech + Gesture

**Scenario**: User says "pick up that" while pointing at object.

**Implementation**:
1. **Voice**: Whisper transcribes "pick up that"
2. **Gesture**: Depth camera detects pointing hand
3. **Fusion**: Identify object at pointing direction
4. **Action**: Grasp detected object

```python
def handle_multimodal_command(voice, depth_image, skeleton):
    """
    Resolve ambiguous commands using gesture.

    Args:
        voice: Transcribed text
        depth_image: Depth map from camera
        skeleton: Human skeleton keypoints

    Returns:
        Resolved command
    """
    if "that" in voice or "there" in voice:
        # Ambiguous reference → use gesture
        pointing_dir = estimate_pointing_direction(skeleton)
        object_at_point = raycast_object(pointing_dir, depth_image)

        resolved_command = voice.replace("that", object_at_point)
        return resolved_command
    else:
        return voice

# Example
voice = "pick up that"
skeleton = detect_human_skeleton(camera_image)
command = handle_multimodal_command(voice, depth_image, skeleton)
print(command)
# Output: "pick up cup"
```

### 10.7.2 Vision + Language Grounding

**Problem**: Map language to visual features.

**Example**:
- Command: "Grasp the **red object**"
- Vision: Detect all objects, identify the red one

**Implementation**:

```python
import clip
import torch
from PIL import Image

# Load CLIP (vision-language model)
model, preprocess = clip.load("ViT-B/32")

# Capture image
image = Image.open("table_scene.jpg")
image_input = preprocess(image).unsqueeze(0)

# Text queries
text_queries = clip.tokenize(["red object", "blue object", "cup", "book"])

# Compute similarities
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_queries)

    # Cosine similarity
    similarity = (image_features @ text_features.T).softmax(dim=-1)

print(similarity)
# Output: [[0.65, 0.10, 0.15, 0.10]]  → "red object" is most likely
```

---

## 10.8 Error Recovery

**Problem**: VLA models hallucinate (output invalid actions).

### 10.8.1 Action Validation

**Check action before execution**:

```python
def validate_action(action, robot_limits):
    """
    Ensure action is safe and feasible.

    Args:
        action: Predicted action
        robot_limits: Dict with joint limits, workspace bounds

    Returns:
        bool: True if valid
    """
    x, y, z = action['position']
    roll, pitch, yaw = action['orientation']

    # Check workspace bounds
    if not (robot_limits['x_min'] <= x <= robot_limits['x_max']):
        return False
    if not (robot_limits['y_min'] <= y <= robot_limits['y_max']):
        return False
    if not (robot_limits['z_min'] <= z <= robot_limits['z_max']):
        return False

    # Check orientation limits
    if abs(roll) > np.pi or abs(pitch) > np.pi or abs(yaw) > np.pi:
        return False

    return True

# Example
action = model.predict(image, "pick up the cup")

if validate_action(action, robot_limits):
    robot.execute(action)
else:
    print("Invalid action, asking user for clarification")
    ask_user_clarification()
```

### 10.8.2 Failure Detection and Replanning

**Monitor execution**:

```python
def execute_with_monitoring(action):
    """
    Execute action with failure detection.
    """
    robot.execute(action)

    # Wait for completion
    time.sleep(2.0)

    # Check if object grasped
    gripper_force = robot.get_gripper_force()

    if gripper_force < 0.5:  # N (no contact)
        print("Grasp failed, replanning...")

        # Ask LLM to replan
        replan_prompt = "The grasp failed (gripper is empty). Suggest alternative approach."
        new_plan = llm.generate(replan_prompt)

        execute_actions(new_plan)
```

---

## 10.9 Safety and Alignment

### 10.9.1 Human-in-the-Loop Confirmation

**Critical actions require approval**:

```python
CRITICAL_ACTIONS = ['navigate_to', 'grasp_sharp_object']

def execute_action_safe(action):
    if action['action'] in CRITICAL_ACTIONS:
        # Ask user for confirmation
        print(f"About to execute: {action}")
        confirm = input("Confirm? (y/n): ")

        if confirm.lower() != 'y':
            print("Action cancelled by user")
            return

    robot.execute(action)
```

### 10.9.2 Action Constraints

**Physical constraints** (prevent collisions):

```python
# Forbidden zones (e.g., human workspace)
FORBIDDEN_ZONES = [
    {'x': [-1.0, 0.0], 'y': [-0.5, 0.5], 'z': [0.0, 2.0]}  # Behind robot
]

def is_in_forbidden_zone(position):
    x, y, z = position

    for zone in FORBIDDEN_ZONES:
        if (zone['x'][0] <= x <= zone['x'][1] and
            zone['y'][0] <= y <= zone['y'][1] and
            zone['z'][0] <= z <= zone['z'][1]):
            return True

    return False
```

---

## 10.10 Full System Integration

**Complete voice-controlled robot**:

```python
import rclpy
from rclpy.node import Node
import whisper
import openai
from openvla import OpenVLA

class VoiceControlledRobot(Node):
    def __init__(self):
        super().__init__('voice_controlled_robot')

        # Load models
        self.whisper_model = whisper.load_model("base")
        self.vla_model = OpenVLA.from_pretrained("openvla-7b")
        self.executor = ActionExecutor()

        # Timer for voice input
        self.timer = self.create_timer(5.0, self.process_voice_command)

    def process_voice_command(self):
        # 1. Record voice
        record_audio("command.wav", duration=3)

        # 2. Transcribe
        result = self.whisper_model.transcribe("command.wav")
        user_command = result["text"]
        self.get_logger().info(f"Heard: {user_command}")

        # 3. Plan with GPT-4
        actions = plan_actions(user_command)
        self.get_logger().info(f"Planned {len(actions)} actions")

        # 4. Execute each action
        for action in actions:
            if action['action'] == 'grasp_object':
                # Use VLA for grasping
                image = capture_camera()
                vla_action = self.vla_model.predict(image, f"grasp {action['object']}")
                robot.execute_grasp(vla_action)
            else:
                # Use traditional action executor
                self.executor.execute_action(action)

        self.get_logger().info("Command completed!")

def main():
    rclpy.init()
    robot = VoiceControlledRobot()
    rclpy.spin(robot)
```

---

## Exercises

### Exercise 10.1: Voice Interface

**Goal**: Implement voice-controlled robot (pick-and-place).

**Requirements**:
1. Use Whisper for voice input
2. Text-to-speech feedback (e.g., pyttsx3)
3. Support 5 commands: navigate, grasp, place, open_gripper, close_gripper

**Expected**: Robot responds to voice in <3 seconds.

### Exercise 10.2: GPT-4 Task Planning

**Goal**: Use GPT-4 to plan multi-step task in Gazebo.

**Task**: "Clean the table by moving all objects to the bin"

**Requirements**:
1. Parse GPT-4 output (JSON)
2. Execute actions sequentially
3. Handle failures (re-plan)

**Success metric**: Complete task in 5 out of 5 trials.

### Exercise 10.3: Fine-Tune OpenVLA

**Goal**: Fine-tune OpenVLA on custom manipulation task.

**Steps**:
1. Collect 10 demonstrations (teleoperation)
2. Fine-tune OpenVLA for 10 epochs
3. Evaluate on 20 test trials
4. Compare to baseline (random actions)

**Expected**: Fine-tuned model achieves >70% success vs <10% for baseline.

---

## Citations

1. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... & Zitkovich, B. (2022). *RT-1: Robotics Transformer for Real-World Control at Scale.* arXiv preprint arXiv:2212.06817.

2. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., ... & Zitkovich, B. (2023). *RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.* Conference on Robot Learning (CoRL).

3. Kim, H., Mees, O., & Levine, S. (2024). *OpenVLA: An Open-Source Vision-Language-Action Model.* arXiv preprint arXiv:2406.09246.

4. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). *Robust Speech Recognition via Large-Scale Weak Supervision (Whisper).* International Conference on Machine Learning (ICML).

5. Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., ... & Kalashnikov, D. (2023). *PaLM-E: An Embodied Multimodal Language Model.* arXiv preprint arXiv:2303.03378.

6. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., ... & Zitkovich, B. (2022). *Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.* Conference on Robot Learning (CoRL).

7. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., ... & Ichter, B. (2022). *Inner Monologue: Embodied Reasoning through Planning with Language Models.* Conference on Robot Learning (CoRL).

8. Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., ... & Zeng, A. (2023). *Code as Policies: Language Model Programs for Embodied Control.* IEEE International Conference on Robotics and Automation (ICRA).

9. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). *Learning Transferable Visual Models From Natural Language Supervision (CLIP).* International Conference on Machine Learning (ICML).

10. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT).* International Conference on Learning Representations (ICLR).

11. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5).* Journal of Machine Learning Research, 21(140), 1-67.

12. Shah, D., Osiński, B., Levine, S., et al. (2023). *RT-X: Open X-Embodiment: Robotic Learning Datasets and RT-X Models.* arXiv preprint arXiv:2310.08864.

13. Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., ... & Sermanet, P. (2023). *Interactive Language: Talking to Robots in Real Time.* IEEE Robotics and Automation Letters, 8(3), 1534-1541.

14. Shridhar, M., Manuelli, L., & Fox, D. (2022). *CLIPort: What and Where Pathways for Robotic Manipulation.* Conference on Robot Learning (CoRL).

15. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., ... & Levine, S. (2022). *BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning.* Conference on Robot Learning (CoRL).

16. Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., ... & Lee, J. (2021). *Transporter Networks: Rearranging the Visual World for Robotic Manipulation.* Conference on Robot Learning (CoRL).

17. Dasari, S., Srirama, M. K., Jain, U., & Gupta, A. (2023). *TidyBot: Personalized Robot Assistance with Large Language Models.* arXiv preprint arXiv:2305.05658.

18. Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., ... & Farhadi, A. (2020). *ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.* IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

19. Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., ... & Zitkovich, B. (2023). *Open X-Embodiment: Robotic Learning Datasets and RT-X Models.* arXiv preprint arXiv:2310.08864.

20. OpenAI. (2023). *GPT-4 Technical Report.* arXiv preprint arXiv:2303.08774.

---

## Hardware Requirements

**Voice Interface** (CPU-only):
- **Microphone**: Any USB microphone ($10-50)
- **Whisper**: Runs on CPU (slow) or GPU (fast)

**VLA Inference**:
- **Minimum**: NVIDIA RTX 3060 (12GB VRAM)
- **Recommended**: NVIDIA Jetson Orin NX (16GB) or RTX 4090 (24GB)

**GPT-4 API**:
- **Internet connection** + OpenAI API key
- **Cost**: ~$0.03 per request (50 tokens output)

---

## Labs

**Lab 10.1: Voice-Controlled Pick-and-Place**

**Repository**: [github.com/Shumailaaijaz/physical-ai-labs](https://github.com/Shumailaaijaz/physical-ai-labs)

**Lab Path**: `labs/chapter-10-voice-control/`

**What's Included**:
- Whisper integration (voice → text)
- GPT-4 action planner
- ROS 2 action executor
- Gazebo simulation world

**Expected Time**: 4 hours

---

**Next Chapter**: [Chapter 11: Capstone Project →](11-capstone-project.mdx)

---

*This textbook is a living document. Found an error? Have a suggestion? Submit an issue or PR at [github.com/Shumailaaijaz/physical-ai-textbook](https://github.com/Shumailaaijaz/physical-ai-textbook)*
