"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8224],{3445:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"10-vision-language-action","title":"Chapter 10: Vision-Language-Action (VLA) Models","description":"\\"Language is the scaffold that connects perception to action.\\"","source":"@site/docs/10-vision-language-action.mdx","sourceDirName":".","slug":"/10-vision-language-action","permalink":"/physical-ai-textbook/docs/10-vision-language-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Shumailaaijaz/physical-ai-textbook/tree/main/docs/10-vision-language-action.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"10-vision-language-action","title":"Chapter 10: Vision-Language-Action (VLA) Models","sidebar_position":10,"part":4,"week":13,"difficulty_levels":["advanced"],"hardware_tracks":["simulation_only","budget_hardware","research_grade"],"citation_count":20,"word_count":8500,"urdu_completeness":0},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Manipulation & Grasping","permalink":"/physical-ai-textbook/docs/09-manipulation-grasping"},"next":{"title":"Chapter 11: Capstone Project","permalink":"/physical-ai-textbook/docs/11-capstone-project"}}');var o=r(4848),t=r(8453);const s={id:"10-vision-language-action",title:"Chapter 10: Vision-Language-Action (VLA) Models",sidebar_position:10,part:4,week:13,difficulty_levels:["advanced"],hardware_tracks:["simulation_only","budget_hardware","research_grade"],citation_count:20,word_count:8500,urdu_completeness:0},a="Chapter 10: Vision-Language-Action (VLA) Models",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"10.1 Introduction: From Chatbots to Robots",id:"101-introduction-from-chatbots-to-robots",level:2},{value:"10.1.1 Evolution",id:"1011-evolution",level:3},{value:"10.2 VLA Architecture",id:"102-vla-architecture",level:2},{value:"10.2.1 Components",id:"1021-components",level:3},{value:"10.2.2 Vision Encoder (ViT)",id:"1022-vision-encoder-vit",level:3},{value:"10.2.3 Language Encoder (T5)",id:"1023-language-encoder-t5",level:3},{value:"10.2.4 Cross-Modal Fusion",id:"1024-cross-modal-fusion",level:3},{value:"10.2.5 Action Decoder",id:"1025-action-decoder",level:3},{value:"10.3 Voice-to-Text (OpenAI Whisper)",id:"103-voice-to-text-openai-whisper",level:2},{value:"10.3.1 Whisper Architecture",id:"1031-whisper-architecture",level:3},{value:"10.3.2 Installation",id:"1032-installation",level:3},{value:"10.3.3 Basic Usage",id:"1033-basic-usage",level:3},{value:"10.3.4 Real-Time Voice Input",id:"1034-real-time-voice-input",level:3},{value:"10.3.5 ROS Integration",id:"1035-ros-integration",level:3},{value:"10.4 Language-to-Action Planning (GPT-4)",id:"104-language-to-action-planning-gpt-4",level:2},{value:"10.4.1 Prompt Engineering",id:"1041-prompt-engineering",level:3},{value:"10.4.2 GPT-4 API Call",id:"1042-gpt-4-api-call",level:3},{value:"10.4.3 Executing Actions with ROS 2",id:"1043-executing-actions-with-ros-2",level:3},{value:"10.5 RT-2: Vision-Language-Action Model",id:"105-rt-2-vision-language-action-model",level:2},{value:"10.5.1 Training Data",id:"1051-training-data",level:3},{value:"10.5.2 Zero-Shot Transfer",id:"1052-zero-shot-transfer",level:3},{value:"10.5.3 Running RT-2 (Requires Google Cloud TPU)",id:"1053-running-rt-2-requires-google-cloud-tpu",level:3},{value:"10.6 OpenVLA (Open-Source Alternative)",id:"106-openvla-open-source-alternative",level:2},{value:"10.6.1 Installation",id:"1061-installation",level:3},{value:"10.6.2 Inference",id:"1062-inference",level:3},{value:"10.6.3 Fine-Tuning on Custom Tasks",id:"1063-fine-tuning-on-custom-tasks",level:3},{value:"10.6.4 Deployment on Jetson Orin",id:"1064-deployment-on-jetson-orin",level:3},{value:"10.7 Multi-Modal Interaction",id:"107-multi-modal-interaction",level:2},{value:"10.7.1 Speech + Gesture",id:"1071-speech--gesture",level:3},{value:"10.7.2 Vision + Language Grounding",id:"1072-vision--language-grounding",level:3},{value:"10.8 Error Recovery",id:"108-error-recovery",level:2},{value:"10.8.1 Action Validation",id:"1081-action-validation",level:3},{value:"10.8.2 Failure Detection and Replanning",id:"1082-failure-detection-and-replanning",level:3},{value:"10.9 Safety and Alignment",id:"109-safety-and-alignment",level:2},{value:"10.9.1 Human-in-the-Loop Confirmation",id:"1091-human-in-the-loop-confirmation",level:3},{value:"10.9.2 Action Constraints",id:"1092-action-constraints",level:3},{value:"10.10 Full System Integration",id:"1010-full-system-integration",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 10.1: Voice Interface",id:"exercise-101-voice-interface",level:3},{value:"Exercise 10.2: GPT-4 Task Planning",id:"exercise-102-gpt-4-task-planning",level:3},{value:"Exercise 10.3: Fine-Tune OpenVLA",id:"exercise-103-fine-tune-openvla",level:3},{value:"Citations",id:"citations",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Labs",id:"labs",level:2}];function d(n){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-10-vision-language-action-vla-models",children:"Chapter 10: Vision-Language-Action (VLA) Models"})}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.em,{children:'"Language is the scaffold that connects perception to action."'})}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Understand Vision-Language-Action"})," model architecture"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Implement voice interfaces"})," with OpenAI Whisper"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Use LLMs to plan robot actions"})," (natural language \u2192 ROS)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deploy RT-2 / OpenVLA"})," for robotic control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Build end-to-end voice-controlled"})," robot"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Estimated Time"}),": 10-12 hours (reading + labs)"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"101-introduction-from-chatbots-to-robots",children:"10.1 Introduction: From Chatbots to Robots"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Large Language Models (LLMs)"})," like GPT-4 and Claude excel at reasoning about text. But can they control robots?"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Challenge"}),": LLMs operate on tokens (words). Robots need ",(0,o.jsx)(e.strong,{children:"actions"})," (joint torques, velocities)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Vision-Language-Action (VLA) Models"})," bridge this gap:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Input"}),': Camera image + text instruction ("pick up the red block")']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Output"}),": Robot action (7D vector: x, y, z, roll, pitch, yaw, gripper)"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"1011-evolution",children:"10.1.1 Evolution"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"2022"}),": RT-1 (Google Robotics)"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"13M parameters"}),"\n",(0,o.jsx)(e.li,{children:"Trained on 130k robot demonstrations"}),"\n",(0,o.jsx)(e.li,{children:"Success rate: 70% on seen tasks"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"2023"}),": RT-2 (Google DeepMind)"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"55B parameters (uses PaLM-E vision-language model)"}),"\n",(0,o.jsx)(e.li,{children:"Trained on web images + robot data"}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Key insight"}),': Web knowledge transfers to robotics (knows "apple" is edible even if never trained to grasp apples)']}),"\n",(0,o.jsx)(e.li,{children:"Success rate: 83% on seen tasks, 62% on novel tasks"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"2024"}),": OpenVLA (UC Berkeley + Stanford)"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"7B parameters"}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Open-source"})," (unlike RT-1/RT-2)"]}),"\n",(0,o.jsx)(e.li,{children:"Trained on Open X-Embodiment dataset (1M+ trajectories, 22 robots)"}),"\n",(0,o.jsx)(e.li,{children:"Runs on NVIDIA Jetson Orin (edge deployment)"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"102-vla-architecture",children:"10.2 VLA Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"1021-components",children:"10.2.1 Components"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                     VLA Model (RT-2)                    \u2502\r\n\u2502                                                         \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\r\n\u2502  \u2502   Vision     \u2502     \u2502   Language   \u2502                \u2502\r\n\u2502  \u2502   Encoder    \u2502     \u2502   Encoder    \u2502                \u2502\r\n\u2502  \u2502   (ViT)      \u2502     \u2502   (T5)       \u2502                \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\r\n\u2502         \u2502                     \u2502                         \u2502\r\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\r\n\u2502                    \u25bc                                    \u2502\r\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\r\n\u2502         \u2502   Cross-Modal        \u2502                        \u2502\r\n\u2502         \u2502   Fusion Layer       \u2502                        \u2502\r\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\r\n\u2502                    \u25bc                                    \u2502\r\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\r\n\u2502         \u2502   Action Decoder     \u2502                        \u2502\r\n\u2502         \u2502   (Transformer)      \u2502                        \u2502\r\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\r\n\u2502                    \u25bc                                    \u2502\r\n\u2502              [x, y, z, R, P, Y, gripper]                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1022-vision-encoder-vit",children:"10.2.2 Vision Encoder (ViT)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Vision Transformer (ViT)"})," processes camera images:"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Patch embedding"}),": Split image into 16\xd716 patches (224\xd7224 image \u2192 196 patches)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Positional encoding"}),": Add position information"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Transformer layers"}),": Self-attention + feedforward (12-24 layers)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Output"}),": 196 patch tokens (each 768D)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Code Example"}),": ViT in PyTorch"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import torch\r\nfrom transformers import ViTModel\r\n\r\n# Load pre-trained ViT\r\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224')\r\n\r\n# Input: RGB image (224x224)\r\nimage = torch.randn(1, 3, 224, 224)\r\n\r\n# Forward pass\r\noutputs = model(pixel_values=image)\r\n\r\n# Last hidden state: (1, 197, 768)\r\n#   197 = 196 patches + 1 [CLS] token\r\n#   768 = embedding dimension\r\nfeatures = outputs.last_hidden_state\r\n\r\nprint(features.shape)\r\n# Output: torch.Size([1, 197, 768])\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1023-language-encoder-t5",children:"10.2.3 Language Encoder (T5)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"T5 (Text-to-Text Transfer Transformer)"})," processes instructions:"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Input"}),': "pick up the red block"\r\n',(0,o.jsx)(e.strong,{children:"Output"}),": Contextualized token embeddings"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from transformers import T5Tokenizer, T5EncoderModel\r\n\r\n# Load T5 encoder\r\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\r\nmodel = T5EncoderModel.from_pretrained('t5-base')\r\n\r\n# Tokenize instruction\r\ntext = \"pick up the red block\"\r\ninputs = tokenizer(text, return_tensors='pt')\r\n\r\n# Encode\r\noutputs = model(**inputs)\r\n\r\n# Last hidden state: (1, seq_len, 768)\r\nlanguage_features = outputs.last_hidden_state\r\n\r\nprint(language_features.shape)\r\n# Output: torch.Size([1, 6, 768])  (6 tokens)\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1024-cross-modal-fusion",children:"10.2.4 Cross-Modal Fusion"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Combine vision and language"})," using cross-attention:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch.nn as nn\r\n\r\nclass CrossModalFusion(nn.Module):\r\n    def __init__(self, dim=768):\r\n        super().__init__()\r\n        self.cross_attn = nn.MultiheadAttention(dim, num_heads=8)\r\n\r\n    def forward(self, vision_features, language_features):\r\n        """\r\n        Args:\r\n            vision_features: (seq_len_v, batch, dim)\r\n            language_features: (seq_len_l, batch, dim)\r\n\r\n        Returns:\r\n            fused_features: (seq_len_v, batch, dim)\r\n        """\r\n        # Cross-attention: language attends to vision\r\n        fused, _ = self.cross_attn(\r\n            query=language_features,\r\n            key=vision_features,\r\n            value=vision_features\r\n        )\r\n        return fused\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1025-action-decoder",children:"10.2.5 Action Decoder"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Transformer decoder"})," predicts action autoregressively:"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Output"}),": 7D action vector"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"(x, y, z)"}),": End-effector position (meters)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"(roll, pitch, yaw)"}),": End-effector orientation (radians)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"gripper"}),": 0 = open, 1 = closed"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Action discretization"})," (RT-2 approach):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Bin continuous actions into 256 discrete values"}),"\n",(0,o.jsx)(e.li,{children:"Example: x \u2208 [-0.5, 0.5] \u2192 bin into 256 levels"}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Why?"})," Transformers predict discrete tokens (easier to train)"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def discretize_action(action, bins=256, action_range=(-0.5, 0.5)):\r\n    """\r\n    Discretize continuous action into bins.\r\n\r\n    Args:\r\n        action: float in action_range\r\n        bins: number of discrete levels\r\n        action_range: (min, max)\r\n\r\n    Returns:\r\n        token: integer in [0, bins-1]\r\n    """\r\n    min_val, max_val = action_range\r\n    normalized = (action - min_val) / (max_val - min_val)  # [0, 1]\r\n    token = int(normalized * (bins - 1))\r\n    return np.clip(token, 0, bins - 1)\r\n\r\n# Example\r\nx_action = 0.25  # meters\r\ntoken = discretize_action(x_action, bins=256, action_range=(-0.5, 0.5))\r\nprint(f"Continuous: {x_action} \u2192 Discrete: {token}")\r\n# Output: Continuous: 0.25 \u2192 Discrete: 191\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"103-voice-to-text-openai-whisper",children:"10.3 Voice-to-Text (OpenAI Whisper)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Whisper"})," is an automatic speech recognition (ASR) model trained on 680,000 hours of multilingual data."]}),"\n",(0,o.jsx)(e.h3,{id:"1031-whisper-architecture",children:"10.3.1 Whisper Architecture"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Encoder"}),": Audio spectrogram \u2192 hidden states"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Decoder"}),": Transformer generates text tokens"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Models"}),": tiny (39M), base (74M), small (244M), medium (769M), large (1550M)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Performance"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Word Error Rate (WER)"}),": 5-10% (comparable to human transcription)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Latency"}),": 0.5-2 seconds (depending on model size)"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"1032-installation",children:"10.3.2 Installation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1033-basic-usage",children:"10.3.3 Basic Usage"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import whisper\r\n\r\n# Load model\r\nmodel = whisper.load_model("base")  # Options: tiny, base, small, medium, large\r\n\r\n# Transcribe audio file\r\nresult = model.transcribe("audio.mp3")\r\n\r\nprint(result["text"])\r\n# Output: "Robot, pick up the red block and place it on the table."\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1034-real-time-voice-input",children:"10.3.4 Real-Time Voice Input"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Capture microphone audio"})," with PyAudio:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import pyaudio\r\nimport wave\r\nimport whisper\r\n\r\ndef record_audio(filename="command.wav", duration=5):\r\n    """\r\n    Record audio from microphone.\r\n\r\n    Args:\r\n        filename: Output WAV file\r\n        duration: Recording duration (seconds)\r\n    """\r\n    CHUNK = 1024\r\n    FORMAT = pyaudio.paInt16\r\n    CHANNELS = 1\r\n    RATE = 16000  # Whisper expects 16kHz\r\n\r\n    p = pyaudio.PyAudio()\r\n\r\n    stream = p.open(format=FORMAT,\r\n                    channels=CHANNELS,\r\n                    rate=RATE,\r\n                    input=True,\r\n                    frames_per_buffer=CHUNK)\r\n\r\n    print("Recording...")\r\n    frames = []\r\n\r\n    for _ in range(0, int(RATE / CHUNK * duration)):\r\n        data = stream.read(CHUNK)\r\n        frames.append(data)\r\n\r\n    print("Done recording.")\r\n\r\n    stream.stop_stream()\r\n    stream.close()\r\n    p.terminate()\r\n\r\n    # Save to file\r\n    wf = wave.open(filename, \'wb\')\r\n    wf.setnchannels(CHANNELS)\r\n    wf.setsampwidth(p.get_sample_size(FORMAT))\r\n    wf.setframerate(RATE)\r\n    wf.writeframes(b\'\'.join(frames))\r\n    wf.close()\r\n\r\n# Record and transcribe\r\nrecord_audio("command.wav", duration=5)\r\nmodel = whisper.load_model("base")\r\nresult = model.transcribe("command.wav")\r\nprint(f"You said: {result[\'text\']}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1035-ros-integration",children:"10.3.5 ROS Integration"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Create ROS 2 node"})," for voice commands:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\n\r\nclass VoiceCommandNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command\')\r\n        self.publisher_ = self.create_publisher(String, \'voice_commands\', 10)\r\n        self.model = whisper.load_model("base")\r\n\r\n        # Record and publish every 5 seconds\r\n        self.timer = self.create_timer(5.0, self.listen_callback)\r\n\r\n    def listen_callback(self):\r\n        # Record audio\r\n        record_audio("temp.wav", duration=3)\r\n\r\n        # Transcribe\r\n        result = self.model.transcribe("temp.wav")\r\n        text = result["text"]\r\n\r\n        # Publish\r\n        msg = String()\r\n        msg.data = text\r\n        self.publisher_.publish(msg)\r\n        self.get_logger().info(f\'Published command: "{text}"\')\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = VoiceCommandNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"104-language-to-action-planning-gpt-4",children:"10.4 Language-to-Action Planning (GPT-4)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Use GPT-4 to decompose"})," natural language commands into robot actions."]}),"\n",(0,o.jsx)(e.h3,{id:"1041-prompt-engineering",children:"10.4.1 Prompt Engineering"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"System prompt"})," defines robot capabilities:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'SYSTEM_PROMPT = """\r\nYou are a robot controller. You have access to the following actions:\r\n\r\n1. navigate_to(location: str) - Move to a named location (e.g., "kitchen", "table")\r\n2. grasp_object(object: str) - Pick up an object by name (e.g., "cup", "book")\r\n3. place_object(location: str) - Put down the currently held object\r\n4. open_gripper() - Open the gripper\r\n5. close_gripper() - Close the gripper\r\n\r\nGiven a user command, output a JSON list of actions to execute.\r\n\r\nExample:\r\nUser: "Bring me a water bottle from the kitchen"\r\nOutput:\r\n[\r\n  {"action": "navigate_to", "location": "kitchen"},\r\n  {"action": "grasp_object", "object": "water bottle"},\r\n  {"action": "navigate_to", "location": "user"}\r\n]\r\n"""\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1042-gpt-4-api-call",children:"10.4.2 GPT-4 API Call"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import openai\r\nimport json\r\n\r\nopenai.api_key = \"YOUR_API_KEY\"\r\n\r\ndef plan_actions(user_command):\r\n    \"\"\"\r\n    Use GPT-4 to plan action sequence.\r\n\r\n    Args:\r\n        user_command: Natural language instruction\r\n\r\n    Returns:\r\n        List of action dictionaries\r\n    \"\"\"\r\n    response = openai.ChatCompletion.create(\r\n        model=\"gpt-4\",\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\r\n            {\"role\": \"user\", \"content\": user_command}\r\n        ],\r\n        temperature=0.0  # Deterministic output\r\n    )\r\n\r\n    # Parse JSON response\r\n    actions_json = response.choices[0].message.content\r\n    actions = json.loads(actions_json)\r\n\r\n    return actions\r\n\r\n# Example\r\ncommand = \"Robot, clean the table by moving all objects to the bin\"\r\nactions = plan_actions(command)\r\n\r\nfor action in actions:\r\n    print(action)\r\n\r\n# Output:\r\n# {'action': 'navigate_to', 'location': 'table'}\r\n# {'action': 'grasp_object', 'object': 'object_1'}\r\n# {'action': 'navigate_to', 'location': 'bin'}\r\n# {'action': 'place_object'}\r\n# {'action': 'navigate_to', 'location': 'table'}\r\n# ...\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1043-executing-actions-with-ros-2",children:"10.4.3 Executing Actions with ROS 2"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Action executor"})," translates GPT-4 output to ROS actions:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom moveit_msgs.action import MoveGroup\r\n\r\nclass ActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'action_executor\')\r\n\r\n        # Action clients\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        self.moveit_client = ActionClient(self, MoveGroup, \'move_action\')\r\n\r\n    def execute_action(self, action):\r\n        """\r\n        Execute a single action.\r\n\r\n        Args:\r\n            action: Dict with \'action\' field and parameters\r\n        """\r\n        action_type = action[\'action\']\r\n\r\n        if action_type == \'navigate_to\':\r\n            self.navigate(action[\'location\'])\r\n        elif action_type == \'grasp_object\':\r\n            self.grasp(action[\'object\'])\r\n        elif action_type == \'place_object\':\r\n            self.place(action.get(\'location\'))\r\n        else:\r\n            self.get_logger().error(f"Unknown action: {action_type}")\r\n\r\n    def navigate(self, location):\r\n        """Navigate to named location."""\r\n        # Map location names to coordinates\r\n        locations = {\r\n            \'kitchen\': (2.0, 1.0, 0.0),\r\n            \'table\': (1.0, 0.0, 0.0),\r\n            \'user\': (0.0, 0.0, 0.0),\r\n        }\r\n\r\n        if location not in locations:\r\n            self.get_logger().error(f"Unknown location: {location}")\r\n            return\r\n\r\n        x, y, theta = locations[location]\r\n\r\n        # Create navigation goal\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = \'map\'\r\n        goal_msg.pose.pose.position.x = x\r\n        goal_msg.pose.pose.position.y = y\r\n        goal_msg.pose.pose.orientation.w = 1.0\r\n\r\n        # Send goal\r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal_msg)\r\n        rclpy.spin_until_future_complete(self, future)\r\n\r\n        self.get_logger().info(f"Navigated to {location}")\r\n\r\n    def grasp(self, object_name):\r\n        """Grasp object by name."""\r\n        # 1. Detect object (vision)\r\n        object_pose = self.detect_object(object_name)\r\n\r\n        # 2. Plan grasp with MoveIt\r\n        # (Similar to Chapter 9 examples)\r\n        self.get_logger().info(f"Grasped {object_name}")\r\n\r\n    def place(self, location):\r\n        """Place currently held object."""\r\n        self.get_logger().info(f"Placed object at {location}")\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"105-rt-2-vision-language-action-model",children:"10.5 RT-2: Vision-Language-Action Model"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RT-2"})," (Robotics Transformer 2) is Google DeepMind's state-of-the-art VLA model."]}),"\n",(0,o.jsx)(e.h3,{id:"1051-training-data",children:"10.5.1 Training Data"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Web images"})," (billions):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"COCO, ImageNet, CC3M"}),"\n",(0,o.jsx)(e.li,{children:'Paired with captions ("a person holding a red apple")'}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Robot demonstrations"})," (130k):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"13 different robots"}),"\n",(0,o.jsx)(e.li,{children:"700+ tasks (pick, place, push, open, close)"}),"\n",(0,o.jsx)(e.li,{children:"Recorded in real kitchens, offices, labs"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key insight"}),": Pre-training on web data provides ",(0,o.jsx)(e.strong,{children:"world knowledge"}),"."]}),"\n",(0,o.jsx)(e.h3,{id:"1052-zero-shot-transfer",children:"10.5.2 Zero-Shot Transfer"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Training"}),': Only trained to grasp "soup cans"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Test"}),': Asked to grasp "ketchup"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Result"}),": Success! (knows ketchup is similar to soup can)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Another example"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Training"}),': Never trained on "endangered animals"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Test"}),': "Move the toy to the endangered animal"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Result"}),": Moves toy to panda plushie (knows pandas are endangered)"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"1053-running-rt-2-requires-google-cloud-tpu",children:"10.5.3 Running RT-2 (Requires Google Cloud TPU)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Note"}),": RT-2 is not open-source. This is a conceptual example."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Hypothetical API\r\nfrom rt2 import RT2Model\r\n\r\n# Load pre-trained model (requires TPU)\r\nmodel = RT2Model.load(\"rt2-55b\")\r\n\r\n# Inference\r\nimage = capture_camera_image()  # 224x224 RGB\r\ninstruction = \"pick up the apple\"\r\n\r\naction = model.predict(image, instruction)\r\n\r\nprint(action)\r\n# Output: {\r\n#   'position': [0.45, 0.12, 0.35],\r\n#   'orientation': [0.0, 0.0, 1.57],\r\n#   'gripper': 1.0  # closed\r\n# }\r\n\r\n# Execute action\r\nrobot.move_to(action['position'], action['orientation'])\r\nrobot.set_gripper(action['gripper'])\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"106-openvla-open-source-alternative",children:"10.6 OpenVLA (Open-Source Alternative)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"OpenVLA"})," is an open-source VLA model (7B parameters)."]}),"\n",(0,o.jsx)(e.h3,{id:"1061-installation",children:"10.6.1 Installation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Clone repository\r\ngit clone https://github.com/openvla/openvla.git\r\ncd openvla\r\n\r\n# Install dependencies\r\npip install -r requirements.txt\r\n\r\n# Download pre-trained checkpoint (7GB)\r\nwget https://huggingface.co/openvla/openvla-7b/resolve/main/openvla-7b.pth\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1062-inference",children:"10.6.2 Inference"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\r\nfrom openvla import OpenVLA\r\nfrom PIL import Image\r\n\r\n# Load model\r\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\r\nmodel = OpenVLA.from_pretrained("openvla-7b").to(device)\r\n\r\n# Capture image\r\nimage = Image.open("camera_view.jpg")\r\n\r\n# Instruction\r\ninstruction = "pick up the red cup"\r\n\r\n# Predict action\r\nwith torch.no_grad():\r\n    action = model.predict(image, instruction)\r\n\r\nprint(f"Position: {action[\'position\']}")\r\nprint(f"Orientation: {action[\'orientation\']}")\r\nprint(f"Gripper: {action[\'gripper\']}")\r\n\r\n# Output:\r\n# Position: [0.452, 0.123, 0.347]\r\n# Orientation: [0.0, 0.0, 1.571]\r\n# Gripper: 0.98\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1063-fine-tuning-on-custom-tasks",children:"10.6.3 Fine-Tuning on Custom Tasks"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Collect demonstrations"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Record demonstrations\r\ndemonstrations = []\r\n\r\nfor episode in range(100):\r\n    # Human teleoperates robot\r\n    obs = []\r\n    actions = []\r\n\r\n    for step in range(50):\r\n        image = capture_camera()\r\n        action = teleop_device.read()  # Joystick, VR controller\r\n\r\n        obs.append(image)\r\n        actions.append(action)\r\n\r\n    demonstrations.append({\r\n        'instruction': \"pick and place\",\r\n        'observations': obs,\r\n        'actions': actions\r\n    })\r\n\r\n# Save dataset\r\ntorch.save(demonstrations, \"custom_dataset.pt\")\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Fine-tune"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"python train.py \\\r\n  --model openvla-7b \\\r\n  --dataset custom_dataset.pt \\\r\n  --epochs 10 \\\r\n  --lr 1e-5 \\\r\n  --output fine_tuned_model.pth\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1064-deployment-on-jetson-orin",children:"10.6.4 Deployment on Jetson Orin"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Optimize for edge inference"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Convert to TensorRT\r\npython export_tensorrt.py \\\r\n  --checkpoint fine_tuned_model.pth \\\r\n  --output openvla_trt.engine \\\r\n  --fp16  # Half-precision (2x faster)\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Run on Jetson"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import tensorrt as trt\r\n\r\n# Load TensorRT engine\r\nwith open("openvla_trt.engine", "rb") as f:\r\n    engine = trt.Runtime(trt.Logger()).deserialize_cuda_engine(f.read())\r\n\r\ncontext = engine.create_execution_context()\r\n\r\n# Inference loop\r\nwhile True:\r\n    image = capture_camera()\r\n    instruction = get_latest_command()  # From voice or text\r\n\r\n    # Run inference\r\n    action = run_trt_inference(context, image, instruction)\r\n\r\n    # Execute\r\n    robot.execute_action(action)\r\n\r\n    time.sleep(0.1)  # 10 Hz\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"107-multi-modal-interaction",children:"10.7 Multi-Modal Interaction"}),"\n",(0,o.jsx)(e.h3,{id:"1071-speech--gesture",children:"10.7.1 Speech + Gesture"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Scenario"}),': User says "pick up that" while pointing at object.']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Voice"}),': Whisper transcribes "pick up that"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gesture"}),": Depth camera detects pointing hand"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fusion"}),": Identify object at pointing direction"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action"}),": Grasp detected object"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def handle_multimodal_command(voice, depth_image, skeleton):\r\n    """\r\n    Resolve ambiguous commands using gesture.\r\n\r\n    Args:\r\n        voice: Transcribed text\r\n        depth_image: Depth map from camera\r\n        skeleton: Human skeleton keypoints\r\n\r\n    Returns:\r\n        Resolved command\r\n    """\r\n    if "that" in voice or "there" in voice:\r\n        # Ambiguous reference \u2192 use gesture\r\n        pointing_dir = estimate_pointing_direction(skeleton)\r\n        object_at_point = raycast_object(pointing_dir, depth_image)\r\n\r\n        resolved_command = voice.replace("that", object_at_point)\r\n        return resolved_command\r\n    else:\r\n        return voice\r\n\r\n# Example\r\nvoice = "pick up that"\r\nskeleton = detect_human_skeleton(camera_image)\r\ncommand = handle_multimodal_command(voice, depth_image, skeleton)\r\nprint(command)\r\n# Output: "pick up cup"\n'})}),"\n",(0,o.jsx)(e.h3,{id:"1072-vision--language-grounding",children:"10.7.2 Vision + Language Grounding"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Problem"}),": Map language to visual features."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:['Command: "Grasp the ',(0,o.jsx)(e.strong,{children:"red object"}),'"']}),"\n",(0,o.jsx)(e.li,{children:"Vision: Detect all objects, identify the red one"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import clip\r\nimport torch\r\nfrom PIL import Image\r\n\r\n# Load CLIP (vision-language model)\r\nmodel, preprocess = clip.load("ViT-B/32")\r\n\r\n# Capture image\r\nimage = Image.open("table_scene.jpg")\r\nimage_input = preprocess(image).unsqueeze(0)\r\n\r\n# Text queries\r\ntext_queries = clip.tokenize(["red object", "blue object", "cup", "book"])\r\n\r\n# Compute similarities\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image_input)\r\n    text_features = model.encode_text(text_queries)\r\n\r\n    # Cosine similarity\r\n    similarity = (image_features @ text_features.T).softmax(dim=-1)\r\n\r\nprint(similarity)\r\n# Output: [[0.65, 0.10, 0.15, 0.10]]  \u2192 "red object" is most likely\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"108-error-recovery",children:"10.8 Error Recovery"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Problem"}),": VLA models hallucinate (output invalid actions)."]}),"\n",(0,o.jsx)(e.h3,{id:"1081-action-validation",children:"10.8.1 Action Validation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Check action before execution"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"def validate_action(action, robot_limits):\r\n    \"\"\"\r\n    Ensure action is safe and feasible.\r\n\r\n    Args:\r\n        action: Predicted action\r\n        robot_limits: Dict with joint limits, workspace bounds\r\n\r\n    Returns:\r\n        bool: True if valid\r\n    \"\"\"\r\n    x, y, z = action['position']\r\n    roll, pitch, yaw = action['orientation']\r\n\r\n    # Check workspace bounds\r\n    if not (robot_limits['x_min'] <= x <= robot_limits['x_max']):\r\n        return False\r\n    if not (robot_limits['y_min'] <= y <= robot_limits['y_max']):\r\n        return False\r\n    if not (robot_limits['z_min'] <= z <= robot_limits['z_max']):\r\n        return False\r\n\r\n    # Check orientation limits\r\n    if abs(roll) > np.pi or abs(pitch) > np.pi or abs(yaw) > np.pi:\r\n        return False\r\n\r\n    return True\r\n\r\n# Example\r\naction = model.predict(image, \"pick up the cup\")\r\n\r\nif validate_action(action, robot_limits):\r\n    robot.execute(action)\r\nelse:\r\n    print(\"Invalid action, asking user for clarification\")\r\n    ask_user_clarification()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1082-failure-detection-and-replanning",children:"10.8.2 Failure Detection and Replanning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Monitor execution"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def execute_with_monitoring(action):\r\n    """\r\n    Execute action with failure detection.\r\n    """\r\n    robot.execute(action)\r\n\r\n    # Wait for completion\r\n    time.sleep(2.0)\r\n\r\n    # Check if object grasped\r\n    gripper_force = robot.get_gripper_force()\r\n\r\n    if gripper_force < 0.5:  # N (no contact)\r\n        print("Grasp failed, replanning...")\r\n\r\n        # Ask LLM to replan\r\n        replan_prompt = "The grasp failed (gripper is empty). Suggest alternative approach."\r\n        new_plan = llm.generate(replan_prompt)\r\n\r\n        execute_actions(new_plan)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"109-safety-and-alignment",children:"10.9 Safety and Alignment"}),"\n",(0,o.jsx)(e.h3,{id:"1091-human-in-the-loop-confirmation",children:"10.9.1 Human-in-the-Loop Confirmation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Critical actions require approval"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"CRITICAL_ACTIONS = ['navigate_to', 'grasp_sharp_object']\r\n\r\ndef execute_action_safe(action):\r\n    if action['action'] in CRITICAL_ACTIONS:\r\n        # Ask user for confirmation\r\n        print(f\"About to execute: {action}\")\r\n        confirm = input(\"Confirm? (y/n): \")\r\n\r\n        if confirm.lower() != 'y':\r\n            print(\"Action cancelled by user\")\r\n            return\r\n\r\n    robot.execute(action)\n"})}),"\n",(0,o.jsx)(e.h3,{id:"1092-action-constraints",children:"10.9.2 Action Constraints"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Physical constraints"})," (prevent collisions):"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Forbidden zones (e.g., human workspace)\r\nFORBIDDEN_ZONES = [\r\n    {'x': [-1.0, 0.0], 'y': [-0.5, 0.5], 'z': [0.0, 2.0]}  # Behind robot\r\n]\r\n\r\ndef is_in_forbidden_zone(position):\r\n    x, y, z = position\r\n\r\n    for zone in FORBIDDEN_ZONES:\r\n        if (zone['x'][0] <= x <= zone['x'][1] and\r\n            zone['y'][0] <= y <= zone['y'][1] and\r\n            zone['z'][0] <= z <= zone['z'][1]):\r\n            return True\r\n\r\n    return False\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"1010-full-system-integration",children:"10.10 Full System Integration"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Complete voice-controlled robot"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport whisper\r\nimport openai\r\nfrom openvla import OpenVLA\r\n\r\nclass VoiceControlledRobot(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_controlled_robot\')\r\n\r\n        # Load models\r\n        self.whisper_model = whisper.load_model("base")\r\n        self.vla_model = OpenVLA.from_pretrained("openvla-7b")\r\n        self.executor = ActionExecutor()\r\n\r\n        # Timer for voice input\r\n        self.timer = self.create_timer(5.0, self.process_voice_command)\r\n\r\n    def process_voice_command(self):\r\n        # 1. Record voice\r\n        record_audio("command.wav", duration=3)\r\n\r\n        # 2. Transcribe\r\n        result = self.whisper_model.transcribe("command.wav")\r\n        user_command = result["text"]\r\n        self.get_logger().info(f"Heard: {user_command}")\r\n\r\n        # 3. Plan with GPT-4\r\n        actions = plan_actions(user_command)\r\n        self.get_logger().info(f"Planned {len(actions)} actions")\r\n\r\n        # 4. Execute each action\r\n        for action in actions:\r\n            if action[\'action\'] == \'grasp_object\':\r\n                # Use VLA for grasping\r\n                image = capture_camera()\r\n                vla_action = self.vla_model.predict(image, f"grasp {action[\'object\']}")\r\n                robot.execute_grasp(vla_action)\r\n            else:\r\n                # Use traditional action executor\r\n                self.executor.execute_action(action)\r\n\r\n        self.get_logger().info("Command completed!")\r\n\r\ndef main():\r\n    rclpy.init()\r\n    robot = VoiceControlledRobot()\r\n    rclpy.spin(robot)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-101-voice-interface",children:"Exercise 10.1: Voice Interface"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Implement voice-controlled robot (pick-and-place)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Use Whisper for voice input"}),"\n",(0,o.jsx)(e.li,{children:"Text-to-speech feedback (e.g., pyttsx3)"}),"\n",(0,o.jsx)(e.li,{children:"Support 5 commands: navigate, grasp, place, open_gripper, close_gripper"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Expected"}),": Robot responds to voice in less than 3 seconds."]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-102-gpt-4-task-planning",children:"Exercise 10.2: GPT-4 Task Planning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Use GPT-4 to plan multi-step task in Gazebo."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Task"}),': "Clean the table by moving all objects to the bin"']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Parse GPT-4 output (JSON)"}),"\n",(0,o.jsx)(e.li,{children:"Execute actions sequentially"}),"\n",(0,o.jsx)(e.li,{children:"Handle failures (re-plan)"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Success metric"}),": Complete task in 5 out of 5 trials."]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-103-fine-tune-openvla",children:"Exercise 10.3: Fine-Tune OpenVLA"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Fine-tune OpenVLA on custom manipulation task."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Collect 10 demonstrations (teleoperation)"}),"\n",(0,o.jsx)(e.li,{children:"Fine-tune OpenVLA for 10 epochs"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate on 20 test trials"}),"\n",(0,o.jsx)(e.li,{children:"Compare to baseline (random actions)"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Expected"}),": Fine-tuned model achieves >70% success vs ",(0,o.jsx)(e.code,{children:"<10%"})," for baseline."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"citations",children:"Citations"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... & Zitkovich, B. (2022). ",(0,o.jsx)(e.em,{children:"RT-1: Robotics Transformer for Real-World Control at Scale."})," arXiv preprint arXiv:2212.06817."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., ... & Zitkovich, B. (2023). ",(0,o.jsx)(e.em,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Kim, H., Mees, O., & Levine, S. (2024). ",(0,o.jsx)(e.em,{children:"OpenVLA: An Open-Source Vision-Language-Action Model."})," arXiv preprint arXiv:2406.09246."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). ",(0,o.jsx)(e.em,{children:"Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)."})," International Conference on Machine Learning (ICML)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., ... & Kalashnikov, D. (2023). ",(0,o.jsx)(e.em,{children:"PaLM-E: An Embodied Multimodal Language Model."})," arXiv preprint arXiv:2303.03378."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., ... & Zitkovich, B. (2022). ",(0,o.jsx)(e.em,{children:"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., ... & Ichter, B. (2022). ",(0,o.jsx)(e.em,{children:"Inner Monologue: Embodied Reasoning through Planning with Language Models."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., ... & Zeng, A. (2023). ",(0,o.jsx)(e.em,{children:"Code as Policies: Language Model Programs for Embodied Control."})," IEEE International Conference on Robotics and Automation (ICRA)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). ",(0,o.jsx)(e.em,{children:"Learning Transferable Visual Models From Natural Language Supervision (CLIP)."})," International Conference on Machine Learning (ICML)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). ",(0,o.jsx)(e.em,{children:"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)."})," International Conference on Learning Representations (ICLR)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). ",(0,o.jsx)(e.em,{children:"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)."})," Journal of Machine Learning Research, 21(140), 1-67."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Shah, D., Osi\u0144ski, B., Levine, S., et al. (2023). ",(0,o.jsx)(e.em,{children:"RT-X: Open X-Embodiment: Robotic Learning Datasets and RT-X Models."})," arXiv preprint arXiv:2310.08864."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., ... & Sermanet, P. (2023). ",(0,o.jsx)(e.em,{children:"Interactive Language: Talking to Robots in Real Time."})," IEEE Robotics and Automation Letters, 8(3), 1534-1541."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Shridhar, M., Manuelli, L., & Fox, D. (2022). ",(0,o.jsx)(e.em,{children:"CLIPort: What and Where Pathways for Robotic Manipulation."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., ... & Levine, S. (2022). ",(0,o.jsx)(e.em,{children:"BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., ... & Lee, J. (2021). ",(0,o.jsx)(e.em,{children:"Transporter Networks: Rearranging the Visual World for Robotic Manipulation."})," Conference on Robot Learning (CoRL)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Dasari, S., Srirama, M. K., Jain, U., & Gupta, A. (2023). ",(0,o.jsx)(e.em,{children:"TidyBot: Personalized Robot Assistance with Large Language Models."})," arXiv preprint arXiv:2305.05658."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., ... & Farhadi, A. (2020). ",(0,o.jsx)(e.em,{children:"ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks."})," IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., ... & Zitkovich, B. (2023). ",(0,o.jsx)(e.em,{children:"Open X-Embodiment: Robotic Learning Datasets and RT-X Models."})," arXiv preprint arXiv:2310.08864."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["OpenAI. (2023). ",(0,o.jsx)(e.em,{children:"GPT-4 Technical Report."})," arXiv preprint arXiv:2303.08774."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Voice Interface"})," (CPU-only):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Microphone"}),": Any USB microphone ($10-50)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Whisper"}),": Runs on CPU (slow) or GPU (fast)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VLA Inference"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Minimum"}),": NVIDIA RTX 3060 (12GB VRAM)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Recommended"}),": NVIDIA Jetson Orin NX (16GB) or RTX 4090 (24GB)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GPT-4 API"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Internet connection"})," + OpenAI API key"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cost"}),": ~$0.03 per request (50 tokens output)"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"labs",children:"Labs"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Lab 10.1: Voice-Controlled Pick-and-Place"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Repository"}),": ",(0,o.jsx)(e.a,{href:"https://github.com/Shumailaaijaz/physical-ai-labs",children:"github.com/Shumailaaijaz/physical-ai-labs"})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Lab Path"}),": ",(0,o.jsx)(e.code,{children:"labs/chapter-10-voice-control/"})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"What's Included"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Whisper integration (voice \u2192 text)"}),"\n",(0,o.jsx)(e.li,{children:"GPT-4 action planner"}),"\n",(0,o.jsx)(e.li,{children:"ROS 2 action executor"}),"\n",(0,o.jsx)(e.li,{children:"Gazebo simulation world"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Expected Time"}),": 4 hours"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,o.jsx)(e.a,{href:"/physical-ai-textbook/docs/11-capstone-project",children:"Chapter 11: Capstone Project \u2192"})]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsxs)(e.em,{children:["This textbook is a living document. Found an error? Have a suggestion? Submit an issue or PR at ",(0,o.jsx)(e.a,{href:"https://github.com/Shumailaaijaz/physical-ai-textbook",children:"github.com/Shumailaaijaz/physical-ai-textbook"})]})})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var i=r(6540);const o={},t=i.createContext(o);function s(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(t.Provider,{value:e},n.children)}}}]);