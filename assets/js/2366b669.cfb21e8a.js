"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3841],{3551:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"01-introduction-to-physical-ai","title":"Chapter 1: Introduction to Physical AI","description":"Learning Objectives","source":"@site/docs/01-introduction-to-physical-ai.mdx","sourceDirName":".","slug":"/01-introduction-to-physical-ai","permalink":"/physical-ai-textbook/docs/01-introduction-to-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Shumailaaijaz/physical-ai-textbook/tree/main/docs/01-introduction-to-physical-ai.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"01-introduction-to-physical-ai","title":"Chapter 1: Introduction to Physical AI","sidebar_position":1,"part":1,"week":"1-2","difficulty_levels":["beginner","intermediate","advanced"],"hardware_tracks":["simulation_only","budget_hardware","research_grade"],"citation_count":25,"word_count":7500,"urdu_completeness":0},"sidebar":"tutorialSidebar","previous":{"title":"Preface","permalink":"/physical-ai-textbook/docs/00-preface"},"next":{"title":"Chapter 2: ROS 2 Fundamentals","permalink":"/physical-ai-textbook/docs/02-ros2-fundamentals"}}');var i=s(4848),t=s(8453);const o={id:"01-introduction-to-physical-ai",title:"Chapter 1: Introduction to Physical AI",sidebar_position:1,part:1,week:"1-2",difficulty_levels:["beginner","intermediate","advanced"],hardware_tracks:["simulation_only","budget_hardware","research_grade"],citation_count:25,word_count:7500,urdu_completeness:0},l="Chapter 1: Introduction to Physical AI",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1.1 What is Physical AI?",id:"11-what-is-physical-ai",level:2},{value:"From Digital to Embodied Intelligence",id:"from-digital-to-embodied-intelligence",level:3},{value:"1.2 The Physical AI Stack",id:"12-the-physical-ai-stack",level:2},{value:"Layer 1: Hardware (The Body)",id:"layer-1-hardware-the-body",level:3},{value:"Layer 2: Low-Level Control (The Reflexes)",id:"layer-2-low-level-control-the-reflexes",level:3},{value:"Layer 3: Middleware (The Nervous System)",id:"layer-3-middleware-the-nervous-system",level:3},{value:"Layer 4: Perception (The Senses)",id:"layer-4-perception-the-senses",level:3},{value:"Layer 5: Task Planning (The Tactics)",id:"layer-5-task-planning-the-tactics",level:3},{value:"Layer 6: High-Level Intelligence (The Strategy)",id:"layer-6-high-level-intelligence-the-strategy",level:3},{value:"1.3 Why Humanoid Robots?",id:"13-why-humanoid-robots",level:2},{value:"The Human-Centered World Hypothesis",id:"the-human-centered-world-hypothesis",level:3},{value:"The Data Abundance Argument",id:"the-data-abundance-argument",level:3},{value:"Industry Adoption",id:"industry-adoption",level:3},{value:"1.4 The Sensor Ecosystem",id:"14-the-sensor-ecosystem",level:2},{value:"1.4.1 Vision (The Eyes)",id:"141-vision-the-eyes",level:3},{value:"1.4.2 LIDAR (Laser Eyes)",id:"142-lidar-laser-eyes",level:3},{value:"1.4.3 IMU (The Inner Ear)",id:"143-imu-the-inner-ear",level:3},{value:"1.4.4 Force/Torque Sensors (The Touch)",id:"144-forcetorque-sensors-the-touch",level:3},{value:"1.5 The Sim-to-Real Gap",id:"15-the-sim-to-real-gap",level:2},{value:"Bridging the Gap: Domain Randomization",id:"bridging-the-gap-domain-randomization",level:3},{value:"1.6 Setting Up Your Development Environment",id:"16-setting-up-your-development-environment",level:2},{value:"Option 1: Windows 10/11 + Docker Desktop (Recommended)",id:"option-1-windows-1011--docker-desktop-recommended",level:3},{value:"Option 2: Cloud Workstation (AWS g5.2xlarge)",id:"option-2-cloud-workstation-aws-g52xlarge",level:3},{value:"Option 3: GitHub Codespaces (Easiest)",id:"option-3-github-codespaces-easiest",level:3},{value:"1.7 Your First Physical AI Program",id:"17-your-first-physical-ai-program",level:2},{value:"1.8 Summary",id:"18-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1.1: Sensor Data Analysis",id:"exercise-11-sensor-data-analysis",level:3},{value:"Exercise 1.2: Simulation vs Reality",id:"exercise-12-simulation-vs-reality",level:3},{value:"Exercise 1.3: Build a Virtual Sensor",id:"exercise-13-build-a-virtual-sensor",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Primary Sources (Peer-Reviewed)",id:"primary-sources-peer-reviewed",level:3},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Industry Reports",id:"industry-reports",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-1-introduction-to-physical-ai",children:"Chapter 1: Introduction to Physical AI"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Define Physical AI"})," and explain how it differs from traditional digital AI systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Identify the key challenges"})," of embodied intelligence (sensor noise, physical constraints, real-time requirements)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Describe the sensor ecosystem"})," for humanoid robots (LIDAR, depth cameras, IMUs, force/torque sensors)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explain why humanoids"})," are uniquely positioned to succeed in human environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set up your development environment"})," (Windows 10/11 + Docker or cloud workstation)"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"11-what-is-physical-ai",children:"1.1 What is Physical AI?"}),"\n",(0,i.jsx)(n.h3,{id:"from-digital-to-embodied-intelligence",children:"From Digital to Embodied Intelligence"}),"\n",(0,i.jsx)(n.p,{children:"For decades, artificial intelligence operated exclusively in the digital realm:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2012"}),": AlexNet achieves superhuman image classification (ImageNet)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2016"}),": AlphaGo defeats world champion Lee Sedol at Go"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2020"}),": GPT-3 generates human-like text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2022"}),": Stable Diffusion creates photorealistic images from text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2023"}),": GPT-4 passes the bar exam, medical licensing exams"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Yet despite these breakthroughs, AI remained ",(0,i.jsx)(n.strong,{children:"disembodied"}),"\u2014confined to screens, servers, and virtual environments."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Physical AI changes this."})}),"\n",(0,i.jsxs)(n.admonition,{title:"Definition: Physical AI",type:"tip",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Physical AI"})," refers to artificial intelligence systems that:"]}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perceive"})," the physical world through sensors (cameras, LIDAR, touch)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reason"})," about physical laws (gravity, friction, momentum, collisions)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Act"})," in the physical world through actuators (motors, grippers, wheels)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learn"})," from physical interactions (reinforcement learning, imitation learning)"]}),"\n"]})]}),"\n",(0,i.jsx)(n.p,{children:"Examples of Physical AI systems:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Autonomous vehicles"}),": Waymo, Tesla FSD navigate city streets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Warehouse robots"}),": Amazon Proteus moves pallets, sorts packages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Surgical robots"}),": da Vinci system assists in minimally invasive surgery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Humanoid robots"}),": Boston Dynamics Atlas, Figure 01, Unitree G1"]}),"\n"]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("summary",{children:["\ud83e\ude9f ",(0,i.jsx)(n.strong,{children:"Windows + WSL2 Quick-Start for This Chapter"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"What you'll install"})," (30 minutes):"]}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Docker Desktop"})," for containerized development"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"(Optional) WSL2"})," for Linux command-line tools"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VS Code"})," with Remote-Containers extension"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Python 3.10+"})," for running code examples"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step-by-Step"}),":"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-powershell",children:"# 1. Download and install Docker Desktop\r\n# https://www.docker.com/products/docker-desktop/\r\n# (Restart required)\r\n\r\n# 2. Verify installation\r\ndocker --version\r\n# Expected: Docker version 24.0.0 or higher\r\n\r\n# 3. Install VS Code\r\n# https://code.visualstudio.com/\r\n\r\n# 4. Install Python (if not already installed)\r\nwinget install Python.Python.3.12\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"All code examples in this chapter run on Windows!"})," No dual-boot or native Ubuntu needed."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("summary",{children:["\u2601\ufe0f ",(0,i.jsx)(n.strong,{children:"One-Click GitHub Codespaces Alternative"})]}),(0,i.jsx)(n.p,{children:"Don't want to install anything locally? Use GitHub Codespaces:"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to: ",(0,i.jsx)(n.a,{href:"https://github.com/Shumailaaijaz/physical-ai-labs",children:"github.com/Shumailaaijaz/physical-ai-labs"})]}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:'"Code"'})," \u2192 ",(0,i.jsx)(n.strong,{children:'"Open with Codespaces"'})]}),"\n",(0,i.jsx)(n.li,{children:"Wait 30 seconds for environment to build"}),"\n",(0,i.jsxs)(n.li,{children:["Run: ",(0,i.jsx)(n.code,{children:"python examples/01-sensor-demo.py"})]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Free tier"}),": 60 hours/month (enough for this entire course)"]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"12-the-physical-ai-stack",children:"1.2 The Physical AI Stack"}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems are built from layered components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  High-Level Intelligence (LLMs, VLA)    \u2502  \u2190 "Clean the room"\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Task Planning (Nav2, MoveIt)           \u2502  \u2190 Path planning, manipulation\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Perception (SLAM, Object Detection)    \u2502  \u2190 Where am I? What do I see?\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Middleware (ROS 2)                     \u2502  \u2190 Communication backbone\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Low-Level Control (PID, MPC)           \u2502  \u2190 Motor commands, balance\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  Hardware (Sensors, Actuators, Compute) \u2502  \u2190 Cameras, motors, Jetson\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,i.jsx)(n.h3,{id:"layer-1-hardware-the-body",children:"Layer 1: Hardware (The Body)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sensors"})," (Perception):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cameras"}),": RGB (color), depth (distance), infrared"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LIDAR"}),": Laser rangefinder for 3D mapping"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU"}),": Inertial Measurement Unit (accelerometer + gyroscope) for balance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Measure contact forces during grasping"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Actuators"})," (Action):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Electric motors"}),": Servo motors (position control), brushless motors (speed control)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hydraulic actuators"}),": High force for heavy lifting (Boston Dynamics Atlas)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pneumatic actuators"}),": Soft robotics (compliant grippers)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Compute"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Workstation"}),": NVIDIA RTX 4070+ for simulation and training"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Edge device"}),": NVIDIA Jetson Orin for real-time inference on the robot"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"layer-2-low-level-control-the-reflexes",children:"Layer 2: Low-Level Control (The Reflexes)"}),"\n",(0,i.jsx)(n.p,{children:"Fast feedback loops (1 kHz - 10 kHz) that maintain stability:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PID controllers"}),": Proportional-Integral-Derivative feedback (motor position control)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Predictive Control (MPC)"}),": Predicts future states, optimizes control sequence"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Zero Moment Point (ZMP)"}),": Keeps humanoid balanced during walking"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),": When a humanoid steps on uneven ground, the IMU detects tilt (10ms), MPC adjusts torso angle (20ms), motors compensate (30ms)\u2014all faster than human reaction time (200ms)."]}),"\n",(0,i.jsx)(n.h3,{id:"layer-3-middleware-the-nervous-system",children:"Layer 3: Middleware (The Nervous System)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS 2"})," (Robot Operating System) acts as the communication backbone:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nodes"}),": Independent processes (e.g., camera_driver, object_detector, path_planner)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Topics"}),": Publish/subscribe messaging (e.g., /camera/image_raw, /cmd_vel)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Services"}),": Request/response calls (e.g., /grasp_object)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actions"}),": Long-running tasks with feedback (e.g., /navigate_to_pose)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We'll dive deep into ROS 2 in ",(0,i.jsx)(n.strong,{children:"Chapter 2"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"layer-4-perception-the-senses",children:"Layer 4: Perception (The Senses)"}),"\n",(0,i.jsx)(n.p,{children:"Transforming raw sensor data into understanding:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SLAM"})," (Simultaneous Localization and Mapping): Build a map while tracking robot position"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Detection"}),": YOLO, Mask R-CNN identify objects (cups, doors, humans)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Segmentation"}),": Label every pixel (floor, wall, obstacle, person)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),": NVIDIA Isaac ROS accelerates VSLAM (Visual SLAM) on Jetson Orin to less than 50ms latency using GPU-optimized algorithms."]}),"\n",(0,i.jsx)(n.h3,{id:"layer-5-task-planning-the-tactics",children:"Layer 5: Task Planning (The Tactics)"}),"\n",(0,i.jsx)(n.p,{children:"High-level goal decomposition:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nav2"}),": Autonomous navigation (A* path planning, obstacle avoidance)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"MoveIt 2"}),": Motion planning for robot arms (inverse kinematics, collision avoidance)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Behavior Trees"}),": Hierarchical task execution (if door is closed, open it first, then walk through)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"layer-6-high-level-intelligence-the-strategy",children:"Layer 6: High-Level Intelligence (The Strategy)"}),"\n",(0,i.jsx)(n.p,{children:"LLMs and VLA models translate natural language to robot actions:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),': "Robot, bring me a water bottle from the fridge"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM reasoning"}),":","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to kitchen"}),"\n",(0,i.jsx)(n.li,{children:"Open fridge door"}),"\n",(0,i.jsx)(n.li,{children:"Detect water bottles (vision)"}),"\n",(0,i.jsx)(n.li,{children:"Grasp a bottle"}),"\n",(0,i.jsx)(n.li,{children:"Navigate back to user"}),"\n",(0,i.jsx)(n.li,{children:"Hand over bottle"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Sequence of ROS 2 action calls"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We'll implement this in ",(0,i.jsx)(n.strong,{children:"Chapter 10 (Vision-Language-Action)"}),"."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"13-why-humanoid-robots",children:"1.3 Why Humanoid Robots?"}),"\n",(0,i.jsx)(n.h3,{id:"the-human-centered-world-hypothesis",children:"The Human-Centered World Hypothesis"}),"\n",(0,i.jsx)(n.p,{children:"Our entire civilization is optimized for the human body:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Environment"})}),(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Designed For"})}),(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Robot Challenge"})})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Staircases"}),(0,i.jsx)(n.td,{children:"Bipedal legs"}),(0,i.jsx)(n.td,{children:"Wheeled robots cannot climb"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Doorknobs"}),(0,i.jsx)(n.td,{children:"Five-fingered hands"}),(0,i.jsx)(n.td,{children:"Grippers need dexterity"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Kitchen counters"}),(0,i.jsx)(n.td,{children:"1.5m height"}),(0,i.jsx)(n.td,{children:"Manipulators need reach"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Car steering wheels"}),(0,i.jsx)(n.td,{children:"Two hands + foot pedals"}),(0,i.jsx)(n.td,{children:"Teleportation arms need coordination"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Build robots with human morphology."]}),"\n",(0,i.jsx)(n.h3,{id:"the-data-abundance-argument",children:"The Data Abundance Argument"}),"\n",(0,i.jsxs)(n.p,{children:["Training embodied AI requires ",(0,i.jsx)(n.strong,{children:"demonstration data"}),"\u2014examples of tasks being performed in the real world."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Humans generate this data constantly"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"YouTube: 500 hours of video uploaded/minute (cooking, assembling furniture, walking)"}),"\n",(0,i.jsx)(n.li,{children:"Ego4D dataset: 3,670 hours of first-person video (Meta AI, 2022)"}),"\n",(0,i.jsx)(n.li,{children:"Every human action is a potential training example for imitation learning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key insight"}),": Humanoid robots can directly learn from human demonstrations because they share our body structure (Anthropic, 2023)."]}),"\n",(0,i.jsx)(n.p,{children:"Example: Train a humanoid to fold laundry by watching 10,000 YouTube videos of humans folding clothes \u2192 Vision-Language-Action model (RT-2) maps video \u2192 motor commands."}),"\n",(0,i.jsx)(n.h3,{id:"industry-adoption",children:"Industry Adoption"}),"\n",(0,i.jsx)(n.p,{children:"Major companies investing in humanoids:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tesla"}),": Optimus robot (announced 2023, trained on Tesla's video data)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Figure AI"}),": Figure 01 (backed by $675M from Microsoft, NVIDIA, OpenAI)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agility Robotics"}),": Digit (working in Amazon warehouses)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Boston Dynamics"}),": Atlas (hydraulic bipedal robot, 2013-present)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Unitree"}),": G1 Humanoid ($16k, most affordable research platform)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA CEO Jensen Huang"})," (GTC 2024): ",(0,i.jsx)(n.em,{children:'"The next wave of AI is Physical AI. Humanoid robots will be the most important application of AI."'})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"14-the-sensor-ecosystem",children:"1.4 The Sensor Ecosystem"}),"\n",(0,i.jsxs)(n.p,{children:["Humanoid robots need ",(0,i.jsx)(n.strong,{children:"multimodal sensing"})," to perceive the world:"]}),"\n",(0,i.jsx)(n.h3,{id:"141-vision-the-eyes",children:"1.4.1 Vision (The Eyes)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"RGB Cameras"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Color images for object recognition, scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware"}),": Intel RealSense D435i (640\xd7480 @ 90fps), Logitech C920 (1080p @ 30fps)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),': Detect objects ("Is this a cup?"), read text, recognize faces']}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Depth Cameras"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Measure distance to every pixel (3D point cloud)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stereo"}),": Two cameras + triangulation (like human eyes)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structured light"}),": Project IR pattern, measure distortion (Kinect)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Time-of-Flight (ToF)"}),": Measure IR laser reflection time (RealSense L515)"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),": Obstacle avoidance, 3D mapping, grasp planning"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Example: Reading Depth from RealSense D435i"})}),"\n",(0,i.jsx)(n.admonition,{title:"For Beginners",type:"info",children:(0,i.jsxs)(n.p,{children:["This code uses the ",(0,i.jsx)(n.code,{children:"pyrealsense2"})," library to connect to an Intel RealSense camera and read depth data."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pyrealsense2 as rs\r\nimport numpy as np\r\n\r\n# Windows 10/11: RealSense SDK works natively (no WSL2 needed)\r\n# Install: pip install pyrealsense2\r\n\r\n# Configure RealSense pipeline\r\npipeline = rs.pipeline()\r\nconfig = rs.config()\r\nconfig.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\r\nconfig.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\r\n\r\n# Start streaming\r\npipeline.start(config)\r\n\r\ntry:\r\n    frames = pipeline.wait_for_frames()\r\n    depth_frame = frames.get_depth_frame()\r\n    color_frame = frames.get_color_frame()\r\n\r\n    # Convert to NumPy arrays\r\n    depth_image = np.asanyarray(depth_frame.get_data())  # Shape: (480, 640), units: mm\r\n    color_image = np.asanyarray(color_frame.get_data())  # Shape: (480, 640, 3)\r\n\r\n    # Measure distance at center pixel\r\n    center_depth = depth_image[240, 320]  # mm\r\n    print(f"Object at center is {center_depth / 1000:.2f} meters away")\r\n\r\nfinally:\r\n    pipeline.stop()\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Object at center is 1.23 meters away\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"Advanced",type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Optimization tip"}),": Use ",(0,i.jsx)(n.code,{children:"rs.align"})," to align depth and color frames for pixel-perfect correspondence. Enable post-processing filters (decimation, spatial, temporal) to reduce noise."]})}),"\n",(0,i.jsx)(n.h3,{id:"142-lidar-laser-eyes",children:"1.4.2 LIDAR (Laser Eyes)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": High-precision 3D mapping (\xb12cm accuracy)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Emit laser pulses (905nm or 1550nm wavelength)"}),"\n",(0,i.jsx)(n.li,{children:"Measure time-of-flight: Distance = (Speed of Light \xd7 Time) / 2"}),"\n",(0,i.jsx)(n.li,{children:"Rotate laser (360\xb0) to scan entire environment"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hardware"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Budget"}),": RPLIDAR A1 ($99, 2D scanning, 12m range)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mid-range"}),": Livox Mid-360 ($599, 3D scanning, 70m range)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Premium"}),": Velodyne VLP-16 ($4,000, 3D scanning, 100m range, 16 laser beams)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),": Autonomous navigation (detect walls, furniture), SLAM"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Example: RPLIDAR Scan (Simulated)"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Inside WSL2 terminal (if using RPLIDAR on Windows):\r\n# Install driver: pip install rplidar-roboticia\r\n\r\nfrom rplidar import RPLidar\r\nimport numpy as np\r\n\r\n# Windows note: RPLIDAR connects via USB serial (COM3, COM4, etc.)\r\n# WSL2 can access serial ports via /dev/ttyUSB0 (use usbipd-win)\r\n\r\nlidar = RPLidar('/dev/ttyUSB0')  # Adjust port\r\n\r\nfor scan in lidar.iter_scans():\r\n    # Each scan is a list of (quality, angle, distance) tuples\r\n    angles = np.array([meas[1] for meas in scan])      # degrees\r\n    distances = np.array([meas[2] for meas in scan])   # mm\r\n\r\n    # Find closest obstacle\r\n    min_dist_idx = np.argmin(distances)\r\n    print(f\"Closest obstacle: {distances[min_dist_idx]:.0f}mm at {angles[min_dist_idx]:.1f}\xb0\")\r\n\r\n    break  # Stop after one scan\r\n\r\nlidar.stop()\r\nlidar.disconnect()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Closest obstacle: 450mm at 32.5\xb0\n"})}),"\n",(0,i.jsxs)(n.admonition,{title:"Simulation Only",type:"note",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"No LIDAR hardware?"})," Use Gazebo's simulated LIDAR sensor:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Inside WSL2 terminal:\r\ndocker run -it osrf/ros:humble-desktop\r\nros2 launch gazebo_ros gazebo.launch.py\r\n# Add LIDAR sensor to robot in Gazebo GUI\n"})})]}),"\n",(0,i.jsx)(n.h3,{id:"143-imu-the-inner-ear",children:"1.4.3 IMU (The Inner Ear)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Measure acceleration and rotation (essential for balance)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration (x, y, z axes) in m/s\xb2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity (roll, pitch, yaw) in rad/s"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Magnetometer"})," (optional): Measures magnetic field (compass heading)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hardware"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Budget"}),": MPU6050 ($2, 6-axis, I2C interface)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mid-range"}),": BNO055 ($20, 9-axis, sensor fusion built-in)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Premium"}),": VectorNav VN-100 ($750, GPS + IMU, tactical-grade accuracy)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Detect if robot is falling (accelerometer spike)"}),"\n",(0,i.jsx)(n.li,{children:"Measure tilt angle (gyroscope integration)"}),"\n",(0,i.jsx)(n.li,{children:"Sensor fusion with vision for robust SLAM"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Example: Reading IMU Data (BNO055)"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Inside WSL2 terminal or native Python on Windows:\r\n# Install: pip install adafruit-circuitpython-bno055\r\n\r\nimport board\r\nimport adafruit_bno055\r\n\r\n# Windows note: Use USB-to-I2C adapter (e.g., FT232H breakout)\r\n# Or use Jetson Orin\'s built-in I2C pins\r\n\r\ni2c = board.I2C()\r\nsensor = adafruit_bno055.BNO055_I2C(i2c)\r\n\r\n# Read orientation (Euler angles)\r\nheading, roll, pitch = sensor.euler  # degrees\r\n\r\n# Read angular velocity\r\ngyro_x, gyro_y, gyro_z = sensor.gyro  # rad/s\r\n\r\n# Read linear acceleration\r\naccel_x, accel_y, accel_z = sensor.acceleration  # m/s\xb2\r\n\r\nprint(f"Robot orientation: Roll={roll:.1f}\xb0, Pitch={pitch:.1f}\xb0, Yaw={heading:.1f}\xb0")\r\nprint(f"Angular velocity: {gyro_x:.2f}, {gyro_y:.2f}, {gyro_z:.2f} rad/s")\r\n\r\n# Detect if robot is falling (acceleration spike > 2g)\r\nif abs(accel_z) > 19.6:  # 2g = 19.6 m/s\xb2\r\n    print("\u26a0\ufe0f WARNING: Robot falling detected!")\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Robot orientation: Roll=2.3\xb0, Pitch=-1.5\xb0, Yaw=45.2\xb0\r\nAngular velocity: 0.01, -0.02, 0.00 rad/s\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"Advanced",type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sensor fusion"}),": Combine IMU + camera + LIDAR for robust state estimation. Use Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF). NVIDIA Isaac ROS provides optimized Visual-Inertial Odometry (VIO) nodes."]})}),"\n",(0,i.jsx)(n.h3,{id:"144-forcetorque-sensors-the-touch",children:"1.4.4 Force/Torque Sensors (The Touch)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Measure contact forces during grasping and manipulation"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"How it works"}),": Strain gauges embedded in robot joints measure force (N) and torque (N\u22c5m)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hardware"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Budget"}),": FSR (Force-Sensitive Resistor) pads ($5, binary touch detection)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mid-range"}),": ATI Mini40 ($2,500, 6-axis F/T sensor, \xb140N, \xb12N\u22c5m)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Premium"}),": ATI Gamma ($8,000, \xb165N, \xb15N\u22c5m, used in da Vinci surgical robot)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Gentle grasping (detect when fingers touch object, apply just enough force)"}),"\n",(0,i.jsx)(n.li,{children:"Slip detection (if object is sliding, increase grip force)"}),"\n",(0,i.jsx)(n.li,{children:'Compliance control (allow robot hand to "give" when pushing against resistance)'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Example: Simulated Grasp Force Control"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass GripperController:\r\n    def __init__(self, max_force=50.0):  # Newtons\r\n        self.max_force = max_force\r\n        self.current_force = 0.0\r\n\r\n    def grasp_object(self, target_force=10.0):\r\n        """Apply force until target is reached or object slips"""\r\n        print(f"Target grasp force: {target_force}N")\r\n\r\n        while self.current_force < target_force:\r\n            # Simulate force increase (in real robot, read from F/T sensor)\r\n            self.current_force += 0.5\r\n            print(f"  Current force: {self.current_force:.1f}N")\r\n\r\n            # Check for slip (in real robot, detect force drop)\r\n            if self.detect_slip():\r\n                print("\u26a0\ufe0f Object slipping! Increasing force...")\r\n                target_force += 2.0\r\n\r\n            if self.current_force > self.max_force:\r\n                print("\u274c Max force reached! Object too heavy or stuck")\r\n                return False\r\n\r\n        print(f"\u2705 Grasp successful at {self.current_force:.1f}N")\r\n        return True\r\n\r\n    def detect_slip(self):\r\n        # Simplified: Random slip with 10% probability\r\n        return np.random.rand() < 0.1\r\n\r\n# Example usage\r\ngripper = GripperController()\r\ngripper.grasp_object(target_force=15.0)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Target grasp force: 15.0N\r\n  Current force: 0.5N\r\n  Current force: 1.0N\r\n  ...\r\n  Current force: 14.5N\r\n\u2705 Grasp successful at 15.0N\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"Research Grade",type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tactile sensing"}),": Advanced robots use tactile sensors (e.g., GelSight) that provide high-resolution contact geometry. Meta's ReSkin sensor (2022) measures shear forces and vibration for texture recognition."]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"15-the-sim-to-real-gap",children:"1.5 The Sim-to-Real Gap"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Challenge"}),": Robots trained in simulation often fail in reality."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why?"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Aspect"})}),(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Simulation"})}),(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Reality"})})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Physics"}),(0,i.jsx)(n.td,{children:"Perfect rigid bodies, no friction noise"}),(0,i.jsx)(n.td,{children:"Flexible materials, unpredictable contact"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Sensors"}),(0,i.jsx)(n.td,{children:"Noiseless, infinite range"}),(0,i.jsx)(n.td,{children:"Noisy, occlusions, calibration drift"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Actuation"}),(0,i.jsx)(n.td,{children:"Instant response, infinite torque"}),(0,i.jsx)(n.td,{children:"Motor delays, torque limits, backlash"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Environment"}),(0,i.jsx)(n.td,{children:"Static, known geometry"}),(0,i.jsx)(n.td,{children:"Dynamic, unknown obstacles, lighting changes"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),": A grasp controller trained in Isaac Sim applies exactly 10.0N. In reality, motor backlash means actual force varies between 8.5N - 11.5N \u2192 object slips or crushes."]}),"\n",(0,i.jsx)(n.h3,{id:"bridging-the-gap-domain-randomization",children:"Bridging the Gap: Domain Randomization"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technique"}),": Randomize simulation parameters during training to force robustness."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Randomize"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Object mass (\xb120%)"}),"\n",(0,i.jsx)(n.li,{children:"Surface friction (0.3 - 0.8)"}),"\n",(0,i.jsx)(n.li,{children:"Camera noise (Gaussian \u03c3=5 pixels)"}),"\n",(0,i.jsx)(n.li,{children:"Lighting (brightness, shadows, reflections)"}),"\n",(0,i.jsx)(n.li,{children:"Motor delays (0-50ms)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": Policy learns to handle variability \u2192 transfers better to real world."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Isaac Sim"})," has built-in domain randomization tools (Chapter 6)."]}),"\n",(0,i.jsx)(n.admonition,{title:"Advanced",type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Research frontier"}),": Use real-world data to fine-tune sim-trained policies (Sim2Real2Sim). Collect 1 hour of real robot data \u2192 update simulator physics \u2192 retrain policy \u2192 deploy. Iterative loop closes sim-to-real gap (Tan et al., ICRA 2018)."]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"16-setting-up-your-development-environment",children:"1.6 Setting Up Your Development Environment"}),"\n",(0,i.jsx)(n.h3,{id:"option-1-windows-1011--docker-desktop-recommended",children:"Option 1: Windows 10/11 + Docker Desktop (Recommended)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why this works"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Isaac Sim runs ",(0,i.jsx)(n.strong,{children:"natively on Windows"})," (Omniverse platform)"]}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 runs in Docker containers (Ubuntu 22.04 inside)"}),"\n",(0,i.jsx)(n.li,{children:"Unity Robotics Hub is Windows-native"}),"\n",(0,i.jsx)(n.li,{children:"70% of students already have Windows laptops"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step-by-Step Setup"})," (45 minutes):"]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("summary",{children:[(0,i.jsx)(n.strong,{children:"1. Install Docker Desktop"})," (15 min)"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-powershell",children:'# Download from: https://www.docker.com/products/docker-desktop/\r\n# Run installer, restart Windows\r\n\r\n# Verify installation\r\ndocker --version\r\n# Expected: Docker version 24.0.7, build afdd53b\r\n\r\ndocker run hello-world\r\n# Expected: "Hello from Docker!" message\n'})})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("summary",{children:[(0,i.jsx)(n.strong,{children:"2. Install WSL2 (Optional, but recommended)"})," (10 min)"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-powershell",children:"# Enable WSL2 feature\r\nwsl --install\r\n\r\n# Install Ubuntu 22.04\r\nwsl --install -d Ubuntu-22.04\r\n\r\n# Set as default\r\nwsl --set-default Ubuntu-22.04\r\n\r\n# Inside WSL2 terminal:\r\nwsl -d Ubuntu-22.04\r\nlsb_release -a\r\n# Expected: Ubuntu 22.04.3 LTS\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why WSL2?"})," Some ROS 2 tools (like ",(0,i.jsx)(n.code,{children:"ros2 doctor"}),") work better in Linux CLI. But 90% of this course works fine in pure Docker."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("summary",{children:[(0,i.jsx)(n.strong,{children:"3. Install NVIDIA Omniverse (for Isaac Sim)"})," (20 min)"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-powershell",children:'# Download Omniverse Launcher: https://www.nvidia.com/en-us/omniverse/download/\r\n# Run installer\r\n\r\n# Inside Omniverse Launcher:\r\n# Library \u2192 Install "Isaac Sim" (requires RTX GPU)\r\n\r\n# Launch Isaac Sim\r\n# Expected: Omniverse window opens, USD scene loads\n'})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": Isaac Sim requires ",(0,i.jsx)(n.strong,{children:"NVIDIA RTX GPU"})," (2060 or higher). If you don't have one, use ",(0,i.jsx)(n.strong,{children:"cloud workstation"})," (see Tier 1 in Preface) or skip to ROS 2 chapters."]})]}),"\n",(0,i.jsx)(n.h3,{id:"option-2-cloud-workstation-aws-g52xlarge",children:"Option 2: Cloud Workstation (AWS g5.2xlarge)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for"}),": Students without RTX GPU or weak laptops."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Setup"})," (30 minutes):"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create AWS Account"})," (free tier available)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Launch EC2 instance"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Instance type: ",(0,i.jsx)(n.code,{children:"g5.2xlarge"})," (A10G GPU, 24GB VRAM, 8 vCPU, 32 GB RAM)"]}),"\n",(0,i.jsx)(n.li,{children:"AMI: Deep Learning AMI (Ubuntu 22.04) - includes CUDA, Docker, NVIDIA drivers"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 100 GB EBS volume"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Connect via SSH"}),":","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ssh -i your-key.pem ubuntu@<instance-public-ip>\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Install Isaac Sim"})," (Omniverse Cloud):","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Follow NVIDIA Cloud setup guide (Chapter 6)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cost"}),": ~$1.50/hour (spot instances ~$0.50/hour)"]}),"\n",(0,i.jsx)(n.admonition,{title:"Budget Tip",type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Save money"}),": Use spot instances (70% cheaper), only run when actively working, stop instance when done (storage still costs ~$10/month)."]})}),"\n",(0,i.jsx)(n.h3,{id:"option-3-github-codespaces-easiest",children:"Option 3: GitHub Codespaces (Easiest)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for"}),": Quick start, no installation, works on any laptop (even Chromebooks)."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Setup"})," (2 minutes):"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to: ",(0,i.jsx)(n.a,{href:"https://github.com/Shumailaaijaz/physical-ai-labs",children:"github.com/Shumailaaijaz/physical-ai-labs"})]}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:'"Code"'})," \u2192 ",(0,i.jsx)(n.strong,{children:'"Create codespace on main"'})]}),"\n",(0,i.jsx)(n.li,{children:"Wait 30 seconds for devcontainer to build"}),"\n",(0,i.jsxs)(n.li,{children:["Inside terminal: ",(0,i.jsx)(n.code,{children:"ros2 run demo_nodes_cpp talker"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No GPU (can't run Isaac Sim)"}),"\n",(0,i.jsx)(n.li,{children:"60 hours/month free (GitHub Pro: 180 hours)"}),"\n",(0,i.jsx)(n.li,{children:"Max 4 cores, 8 GB RAM"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Good for"}),": ROS 2 basics (Chapters 2-5), simple Gazebo simulations"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"17-your-first-physical-ai-program",children:"1.7 Your First Physical AI Program"}),"\n",(0,i.jsx)(n.p,{children:"Let's build a simple obstacle avoidance behavior using simulated LIDAR data."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scenario"}),": Robot scans environment with LIDAR \u2192 detects obstacles \u2192 turns away from closest obstacle."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Code"})," (",(0,i.jsx)(n.code,{children:"examples/01-obstacle-avoidance.py"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nclass SimpleRobot:\r\n    def __init__(self):\r\n        self.position = np.array([0.0, 0.0])  # meters\r\n        self.heading = 0.0  # radians\r\n        self.speed = 0.5  # m/s\r\n\r\n    def sense_lidar(self, obstacles):\r\n        """Simulate LIDAR: measure distance to obstacles"""\r\n        angles = np.linspace(0, 2*np.pi, 360)  # 360\xb0 scan\r\n        distances = []\r\n\r\n        for angle in angles:\r\n            # Ray from robot in direction (heading + angle)\r\n            ray_angle = self.heading + angle\r\n            ray_dir = np.array([np.cos(ray_angle), np.sin(ray_angle)])\r\n\r\n            # Find closest obstacle hit\r\n            min_dist = 10.0  # Max range\r\n            for obs_pos, obs_radius in obstacles:\r\n                # Distance to obstacle center\r\n                to_obs = obs_pos - self.position\r\n                proj = np.dot(to_obs, ray_dir)\r\n                if proj < 0:\r\n                    continue  # Behind robot\r\n\r\n                # Perpendicular distance to ray\r\n                perp_dist = np.linalg.norm(to_obs - proj * ray_dir)\r\n                if perp_dist < obs_radius:\r\n                    min_dist = min(min_dist, proj)\r\n\r\n            distances.append(min_dist)\r\n\r\n        return np.array(angles), np.array(distances)\r\n\r\n    def avoid_obstacle(self, angles, distances):\r\n        """Turn away from closest obstacle"""\r\n        min_idx = np.argmin(distances)\r\n        closest_angle = angles[min_idx]\r\n\r\n        if distances[min_idx] < 1.0:  # Obstacle within 1 meter\r\n            # Turn opposite direction\r\n            turn_angle = closest_angle + np.pi\r\n            self.heading += 0.1 * np.sin(turn_angle)  # Proportional turn\r\n            print(f"\u26a0\ufe0f Obstacle at {distances[min_idx]:.2f}m, turning {np.degrees(turn_angle):.0f}\xb0")\r\n        else:\r\n            print("\u2705 Path clear, moving forward")\r\n\r\n    def move(self):\r\n        """Move forward in current heading"""\r\n        self.position += self.speed * np.array([np.cos(self.heading), np.sin(self.heading)])\r\n\r\n# Simulation\r\nrobot = SimpleRobot()\r\nobstacles = [\r\n    (np.array([2.0, 1.0]), 0.5),  # (position, radius)\r\n    (np.array([3.0, -0.5]), 0.3),\r\n]\r\n\r\nfor step in range(10):\r\n    print(f"\\n--- Step {step} ---")\r\n    print(f"Position: {robot.position}, Heading: {np.degrees(robot.heading):.1f}\xb0")\r\n\r\n    angles, distances = robot.sense_lidar(obstacles)\r\n    robot.avoid_obstacle(angles, distances)\r\n    robot.move()\r\n\r\nprint(f"\\nFinal position: {robot.position}")\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"--- Step 0 ---\r\nPosition: [0. 0.], Heading: 0.0\xb0\r\n\u2705 Path clear, moving forward\r\n\r\n--- Step 1 ---\r\nPosition: [0.5 0. ], Heading: 0.0\xb0\r\n\u2705 Path clear, moving forward\r\n\r\n--- Step 2 ---\r\nPosition: [1. 0.], Heading: 0.0\xb0\r\n\u26a0\ufe0f Obstacle at 0.95m, turning 178\xb0\r\n...\n"})}),"\n",(0,i.jsxs)(n.admonition,{title:"For Beginners",type:"info",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"What this code does"}),":"]}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sense_lidar()"}),": Casts 360 rays, measures distance to obstacles (like real LIDAR)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"avoid_obstacle()"}),": Finds closest obstacle, turns away"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"move()"}),": Updates position based on heading and speed"]}),"\n"]}),(0,i.jsxs)(n.p,{children:["This is a simplified version of ",(0,i.jsx)(n.strong,{children:"reactive navigation"}),"\u2014robots that respond to immediate sensor input without planning."]})]}),"\n",(0,i.jsxs)(n.admonition,{title:"Advanced",type:"tip",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Limitations of reactive control"}),":"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Gets stuck in local minima (U-shaped obstacles)"}),"\n",(0,i.jsx)(n.li,{children:"No goal-directed behavior (just avoids, doesn't navigate)"}),"\n",(0,i.jsx)(n.li,{children:"No memory (doesn't build map)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better approach"}),": Use ",(0,i.jsx)(n.strong,{children:"deliberative planning"})," (Nav2 with costmaps, A* path planning). We'll cover this in Chapter 7 (Isaac ROS Navigation)."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"18-summary",children:"1.8 Summary"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Physical AI"})," bridges digital intelligence and the physical world through:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embodiment"}),": Sensors (cameras, LIDAR, IMU) + actuators (motors, grippers)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physics-aware reasoning"}),": Gravity, friction, collisions constrain actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time control"}),": Fast feedback loops (1-10 kHz) maintain stability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sim-to-real transfer"}),": Train in simulation, deploy to real robots"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Humanoid robots"})," are uniquely positioned because:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Human-centered environments (stairs, doorknobs) match their morphology"}),"\n",(0,i.jsx)(n.li,{children:"Abundant human demonstration data (YouTube, Ego4D) enables imitation learning"}),"\n",(0,i.jsx)(n.li,{children:"Industry momentum (Tesla Optimus, Figure 01, Unitree G1) accelerating development"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"This course"})," teaches you to build Physical AI systems using:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2"}),": Middleware for distributed robot control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gazebo/Unity/Isaac Sim"}),": Photorealistic simulation for training"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Jetson"}),": Edge AI deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": LLMs that command robots"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-11-sensor-data-analysis",children:"Exercise 1.1: Sensor Data Analysis"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task"}),": Analyze real LIDAR data from a warehouse robot."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Dataset"}),": ",(0,i.jsx)(n.a,{href:"https://github.com/Shumailaaijaz/physical-ai-labs/blob/main/chapter-01/lidar_scan.csv",children:"Download LIDAR scan (CSV)"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Questions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"What is the closest obstacle distance?"}),"\n",(0,i.jsx)(n.li,{children:"At what angle is the closest obstacle?"}),"\n",(0,i.jsx)(n.li,{children:"How many obstacles are within 2 meters?"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import pandas as pd\r\nimport numpy as np\r\n\r\n# Load data\r\ndata = pd.read_csv('lidar_scan.csv')\r\nangles = data['angle'].values  # degrees\r\ndistances = data['distance'].values  # meters\r\n\r\n# TODO: Your analysis here\n"})}),"\n",(0,i.jsx)(n.h3,{id:"exercise-12-simulation-vs-reality",children:"Exercise 1.2: Simulation vs Reality"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task"}),": Compare simulated vs real camera noise."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Capture 100 frames from a webcam pointing at a static scene"}),"\n",(0,i.jsx)(n.li,{children:"Compute pixel-wise standard deviation (measure of noise)"}),"\n",(0,i.jsx)(n.li,{children:"Compare to simulated camera noise (Gaussian \u03c3=5 pixels)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\ncap = cv2.VideoCapture(0)  # Webcam\r\nframes = []\r\n\r\nfor i in range(100):\r\n    ret, frame = cap.read()\r\n    frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\r\n\r\nframes = np.array(frames)\r\nnoise_std = np.std(frames, axis=0)  # Std dev per pixel\r\n\r\nprint(f"Mean pixel noise: {np.mean(noise_std):.2f}")\r\ncap.release()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"exercise-13-build-a-virtual-sensor",children:"Exercise 1.3: Build a Virtual Sensor"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task"}),": Implement a simulated ultrasonic sensor (returns distance to nearest obstacle in a cone)."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Specification"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Sensor range: 0.1m - 4.0m"}),"\n",(0,i.jsx)(n.li,{children:"Beam width: 15\xb0 cone"}),"\n",(0,i.jsx)(n.li,{children:"Update rate: 10 Hz"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Pseudocode"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def sense_ultrasonic(robot_pos, robot_heading, obstacles, max_range=4.0, cone_angle=15):\r\n    # Cast rays in 15\xb0 cone\r\n    # Return distance to nearest obstacle\r\n    pass\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsx)(n.h3,{id:"primary-sources-peer-reviewed",children:"Primary Sources (Peer-Reviewed)"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Kajita, S., et al. (2003). ",(0,i.jsx)(n.em,{children:"Biped walking pattern generation by using preview control of zero-moment point."})," IEEE ICRA. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/ROBOT.2003.1241826",children:"https://doi.org/10.1109/ROBOT.2003.1241826"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,i.jsx)(n.em,{children:"Probabilistic Robotics."})," MIT Press. (Textbook on SLAM, localization, Kalman filters)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Brohan, A., et al. (2023). ",(0,i.jsx)(n.em,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control."})," CoRL 2023. ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"https://arxiv.org/abs/2307.15818"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Radosavovic, I., et al. (2024). ",(0,i.jsx)(n.em,{children:"Humanoid Locomotion as Next Token Prediction."})," arXiv preprint. ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2402.19469",children:"https://arxiv.org/abs/2402.19469"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["ROS 2 Humble Documentation. (2023). ",(0,i.jsx)(n.em,{children:"Core Concepts: Nodes, Topics, Services."})," ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts.html",children:"https://docs.ros.org/en/humble/Concepts.html"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["NVIDIA Isaac Sim Documentation. (2024). ",(0,i.jsx)(n.em,{children:"Getting Started with Isaac Sim."})," ",(0,i.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Intel RealSense SDK. (2024). ",(0,i.jsx)(n.em,{children:"Python API Reference."})," ",(0,i.jsx)(n.a,{href:"https://github.com/IntelRealSense/librealsense/tree/master/wrappers/python",children:"https://github.com/IntelRealSense/librealsense/tree/master/wrappers/python"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"industry-reports",children:"Industry Reports"}),"\n",(0,i.jsxs)(n.ol,{start:"8",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["NVIDIA GTC 2024 Keynote. (March 2024). ",(0,i.jsx)(n.em,{children:"Jensen Huang on Physical AI and Humanoid Robots."})," ",(0,i.jsx)(n.a,{href:"https://www.nvidia.com/gtc/",children:"https://www.nvidia.com/gtc/"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Boston Dynamics. (2023). ",(0,i.jsx)(n.em,{children:"Atlas: The Next Generation of Humanoid Robots."})," ",(0,i.jsx)(n.a,{href:"https://bostondynamics.com/atlas/",children:"https://bostondynamics.com/atlas/"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Unitree Robotics. (2024). ",(0,i.jsx)(n.em,{children:"G1 Humanoid Robot Technical Specifications."})," ",(0,i.jsx)(n.a,{href:"https://www.unitree.com/g1/",children:"https://www.unitree.com/g1/"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-textbook/docs/02-ros2-fundamentals",children:"Chapter 2: ROS 2 Fundamentals \u2192"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Found an error or have a question? Open an issue: ",(0,i.jsx)(n.a,{href:"https://github.com/Shumailaaijaz/physical-ai-textbook/issues",children:"github.com/Shumailaaijaz/physical-ai-textbook/issues"})]})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var r=s(6540);const i={},t=r.createContext(i);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);