"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2399],{2747:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"09-manipulation-grasping","title":"Chapter 9: Manipulation & Grasping","description":"\\"The hand is the visible part of the brain.\\" \u2014 Immanuel Kant","source":"@site/docs/09-manipulation-grasping.mdx","sourceDirName":".","slug":"/09-manipulation-grasping","permalink":"/physical-ai-textbook/docs/09-manipulation-grasping","draft":false,"unlisted":false,"editUrl":"https://github.com/Shumailaaijaz/physical-ai-textbook/tree/main/docs/09-manipulation-grasping.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"id":"09-manipulation-grasping","title":"Chapter 9: Manipulation & Grasping","sidebar_position":9,"part":4,"week":12,"difficulty_levels":["advanced"],"hardware_tracks":["simulation_only","research_grade"],"citation_count":12,"word_count":6800,"urdu_completeness":0},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Legged Locomotion","permalink":"/physical-ai-textbook/docs/08-legged-locomotion"},"next":{"title":"Chapter 10: Vision-Language-Action (VLA) Models","permalink":"/physical-ai-textbook/docs/10-vision-language-action"}}');var i=r(4848),o=r(8453);const t={id:"09-manipulation-grasping",title:"Chapter 9: Manipulation & Grasping",sidebar_position:9,part:4,week:12,difficulty_levels:["advanced"],hardware_tracks:["simulation_only","research_grade"],citation_count:12,word_count:6800,urdu_completeness:0},a="Chapter 9: Manipulation & Grasping",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"9.1 Introduction: Manipulation Challenges",id:"91-introduction-manipulation-challenges",level:2},{value:"9.1.1 State of the Art",id:"911-state-of-the-art",level:3},{value:"9.2 Forward and Inverse Kinematics",id:"92-forward-and-inverse-kinematics",level:2},{value:"9.2.1 Denavit-Hartenberg (DH) Parameters",id:"921-denavit-hartenberg-dh-parameters",level:3},{value:"9.2.2 Forward Kinematics (FK)",id:"922-forward-kinematics-fk",level:3},{value:"9.2.3 Inverse Kinematics (IK)",id:"923-inverse-kinematics-ik",level:3},{value:"9.2.4 Redundancy (7-DOF Arms)",id:"924-redundancy-7-dof-arms",level:3},{value:"9.3 MoveIt 2",id:"93-moveit-2",level:2},{value:"9.3.1 MoveIt Architecture",id:"931-moveit-architecture",level:3},{value:"9.3.2 Installation",id:"932-installation",level:3},{value:"9.3.3 Basic Usage",id:"933-basic-usage",level:3},{value:"9.3.4 Motion Planners",id:"934-motion-planners",level:3},{value:"9.4 Grasp Planning",id:"94-grasp-planning",level:2},{value:"9.4.1 Force Closure",id:"941-force-closure",level:3},{value:"9.4.2 Antipodal Grasps",id:"942-antipodal-grasps",level:3},{value:"9.4.3 GraspNet (Learning-Based)",id:"943-graspnet-learning-based",level:3},{value:"9.5 Force Control",id:"95-force-control",level:2},{value:"9.5.1 Impedance Control",id:"951-impedance-control",level:3},{value:"9.5.2 Admittance Control",id:"952-admittance-control",level:3},{value:"9.6 Visual Servoing",id:"96-visual-servoing",level:2},{value:"9.6.1 Position-Based Visual Servoing (PBVS)",id:"961-position-based-visual-servoing-pbvs",level:3},{value:"9.6.2 Image-Based Visual Servoing (IBVS)",id:"962-image-based-visual-servoing-ibvs",level:3},{value:"9.7 Dual-Arm Manipulation",id:"97-dual-arm-manipulation",level:2},{value:"9.7.1 Coordination",id:"971-coordination",level:3},{value:"9.8 Whole-Body Manipulation (Humanoids)",id:"98-whole-body-manipulation-humanoids",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 9.1: Compare IK Solvers",id:"exercise-91-compare-ik-solvers",level:3},{value:"Exercise 9.2: Stack 3 Blocks",id:"exercise-92-stack-3-blocks",level:3},{value:"Exercise 9.3: Train GraspNet on YCB Objects",id:"exercise-93-train-graspnet-on-ycb-objects",level:3},{value:"Citations",id:"citations",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Labs",id:"labs",level:2}];function d(n){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-9-manipulation--grasping",children:"Chapter 9: Manipulation & Grasping"})}),"\n",(0,i.jsxs)(e.blockquote,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.em,{children:'"The hand is the visible part of the brain." \u2014 Immanuel Kant'})}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Solve inverse kinematics"})," for robot arms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Plan collision-free trajectories"})," with MoveIt 2"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Implement grasp planning"})," algorithms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Use force control"})," for compliant manipulation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Integrate vision"})," (object detection) + manipulation"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Estimated Time"}),": 8-10 hours (reading + labs)"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"91-introduction-manipulation-challenges",children:"9.1 Introduction: Manipulation Challenges"}),"\n",(0,i.jsx)(e.p,{children:"Picking up a cup seems trivial to humans. For robots, it requires solving multiple hard problems:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Challenges"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Redundancy"}),": 7-DOF arm has infinite solutions for the same end-effector pose (elbow up vs down)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Grasp stability"}),": Where to place fingers? (force closure, contact points)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Collision avoidance"}),": Don't hit the table, other objects, or yourself"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Perception uncertainty"}),": Object pose may be off by 1-2cm (enough to fail grasp)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contact forces"}),": Too weak \u2192 object slips; too strong \u2192 object breaks"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"911-state-of-the-art",children:"9.1.1 State of the Art"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Franka Emika Panda"})," (research standard):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"7-DOF arm (redundant)"}),"\n",(0,i.jsx)(e.li,{children:"Parallel-jaw gripper (max 0.08m opening)"}),"\n",(0,i.jsx)(e.li,{children:"Torque sensors (all 7 joints)"}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Price"}),": $25,000"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unitree Z1"})," (budget option):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"6-DOF arm"}),"\n",(0,i.jsx)(e.li,{children:"Gripper with force sensing"}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Price"}),": $3,000"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unitree G1 Humanoid"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Dual 7-DOF arms (whole-body manipulation)"}),"\n",(0,i.jsx)(e.li,{children:"3-finger dexterous hands"}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Use case"}),": Bi-manual tasks (opening jars, folding clothes)"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"92-forward-and-inverse-kinematics",children:"9.2 Forward and Inverse Kinematics"}),"\n",(0,i.jsx)(e.h3,{id:"921-denavit-hartenberg-dh-parameters",children:"9.2.1 Denavit-Hartenberg (DH) Parameters"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"DH Convention"}),": Standard way to describe robot arm geometry."]}),"\n",(0,i.jsx)(e.p,{children:"Each joint has 4 parameters:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"\u03b8"}),": Joint angle (revolute) or displacement (prismatic)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"d"}),": Link offset along z-axis"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"a"}),": Link length along x-axis"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"\u03b1"}),": Link twist (rotation about x-axis)"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example"}),": PUMA 560 arm (classic 6-DOF)"]}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Joint"}),(0,i.jsx)(e.th,{children:"\u03b8"}),(0,i.jsx)(e.th,{children:"d"}),(0,i.jsx)(e.th,{children:"a"}),(0,i.jsx)(e.th,{children:"\u03b1"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"1"}),(0,i.jsx)(e.td,{children:"\u03b8\u2081*"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"90\xb0"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"2"}),(0,i.jsx)(e.td,{children:"\u03b8\u2082*"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"0.4318"}),(0,i.jsx)(e.td,{children:"0\xb0"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"3"}),(0,i.jsx)(e.td,{children:"\u03b8\u2083*"}),(0,i.jsx)(e.td,{children:"0.15005"}),(0,i.jsx)(e.td,{children:"0.0203"}),(0,i.jsx)(e.td,{children:"-90\xb0"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"4"}),(0,i.jsx)(e.td,{children:"\u03b8\u2084*"}),(0,i.jsx)(e.td,{children:"0.4318"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"90\xb0"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"5"}),(0,i.jsx)(e.td,{children:"\u03b8\u2085*"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"-90\xb0"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"6"}),(0,i.jsx)(e.td,{children:"\u03b8\u2086*"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"0\xb0"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"(*) Variable (controlled by motors)"}),"\n",(0,i.jsx)(e.h3,{id:"922-forward-kinematics-fk",children:"9.2.2 Forward Kinematics (FK)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Given joint angles \u03b8\u2081, \u03b8\u2082, ..., \u03b8\u2099, find end-effector pose."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Homogeneous Transformation Matrix"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"T = T\u2081 T\u2082 T\u2083 T\u2084 T\u2085 T\u2086\r\n\r\nWhere each T\u1d62 is:\r\n[cos(\u03b8\u1d62)  -sin(\u03b8\u1d62)cos(\u03b1\u1d62)   sin(\u03b8\u1d62)sin(\u03b1\u1d62)   a\u1d62cos(\u03b8\u1d62)]\r\n[sin(\u03b8\u1d62)   cos(\u03b8\u1d62)cos(\u03b1\u1d62)  -cos(\u03b8\u1d62)sin(\u03b1\u1d62)   a\u1d62sin(\u03b8\u1d62)]\r\n[   0          sin(\u03b1\u1d62)           cos(\u03b1\u1d62)           d\u1d62     ]\r\n[   0             0                  0               1      ]\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"})," (using modern robotics library):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nfrom modern_robotics import FKinSpace\r\n\r\n# Screw axes (spatial)\r\nSlist = np.array([\r\n    [0,  0,  1,  0,    0,    0],\r\n    [0,  1,  0, -0.1, 0,    0],\r\n    [0,  1,  0, -0.1, 0,    0.4],\r\n    [0,  1,  0, -0.1, 0,    0.8],\r\n    [0,  0,  1,  0,   -0.8,  0],\r\n    [0,  1,  0, -0.1, 0,    1.0]\r\n]).T\r\n\r\n# Home configuration (end-effector pose at \u03b8 = 0)\r\nM = np.array([\r\n    [1, 0, 0, 1.0],\r\n    [0, 1, 0, 0],\r\n    [0, 0, 1, 0],\r\n    [0, 0, 0, 1]\r\n])\r\n\r\n# Joint angles\r\ntheta = np.array([0, -np.pi/4, 0, np.pi/2, 0, 0])\r\n\r\n# Compute FK\r\nT = FKinSpace(M, Slist, theta)\r\n\r\nprint("End-effector pose:")\r\nprint(T)\r\n# Output:\r\n# [[ 0.707  0.707  0     0.883]\r\n#  [-0.707  0.707  0     0    ]\r\n#  [ 0      0      1     0.283]\r\n#  [ 0      0      0     1    ]]\n'})}),"\n",(0,i.jsx)(e.h3,{id:"923-inverse-kinematics-ik",children:"9.2.3 Inverse Kinematics (IK)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Given desired end-effector pose, find joint angles."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Analytical IK"})," (closed-form):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Only possible for specific arm geometries (6-DOF with spherical wrist)"}),"\n",(0,i.jsx)(e.li,{children:"Fast (microseconds)"}),"\n",(0,i.jsx)(e.li,{children:"Example: PUMA 560, UR5"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Numerical IK"})," (iterative):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Works for any arm"}),"\n",(0,i.jsx)(e.li,{children:"Slower (milliseconds)"}),"\n",(0,i.jsx)(e.li,{children:"Methods: Jacobian pseudoinverse, Newton-Raphson, Levenberg-Marquardt"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),": Numerical IK using PyKDL"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import PyKDL as kdl\r\n\r\n# Define robot chain (6-DOF)\r\nchain = kdl.Chain()\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0, np.pi/2, 0, 0)))\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0.4318, 0, 0, 0)))\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0.0203, -np.pi/2, 0.15005, 0)))\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0, np.pi/2, 0.4318, 0)))\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0, -np.pi/2, 0, 0)))\r\nchain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0, 0, 0, 0)))\r\n\r\n# IK solver\r\nik_solver = kdl.ChainIkSolverPos_LMA(chain)\r\n\r\n# Desired pose\r\ntarget = kdl.Frame(kdl.Rotation.RPY(0, 0, 0), kdl.Vector(0.5, 0.2, 0.3))\r\n\r\n# Initial guess\r\nq_init = kdl.JntArray(6)\r\n\r\n# Solve IK\r\nq_out = kdl.JntArray(6)\r\nresult = ik_solver.CartToJnt(q_init, target, q_out)\r\n\r\nif result >= 0:\r\n    print("IK solution (radians):")\r\n    for i in range(6):\r\n        print(f"  Joint {i+1}: {q_out[i]:.3f}")\r\nelse:\r\n    print("IK failed (target unreachable)")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"924-redundancy-7-dof-arms",children:"9.2.4 Redundancy (7-DOF Arms)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": 7-DOF arm has infinite IK solutions for the same end-effector pose."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Use redundancy for"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Obstacle avoidance"}),": Choose elbow configuration that avoids table"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Joint limit avoidance"}),": Stay away from joint limits (safer)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Secondary tasks"}),": Optimize energy, manipulability"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Null-space projection"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Primary task: Reach target (6D)\r\nJ = compute_jacobian(q)  # 6x7 Jacobian\r\ndq_primary = np.linalg.pinv(J) @ dx_desired\r\n\r\n# Secondary task: Minimize joint deviation from home\r\nq_home = np.array([0, 0, 0, 0, 0, 0, 0])\r\ndq_secondary = 0.1 * (q_home - q)\r\n\r\n# Project secondary task into null space\r\nN = np.eye(7) - np.linalg.pinv(J) @ J  # Null-space projector\r\ndq_total = dq_primary + N @ dq_secondary\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"93-moveit-2",children:"9.3 MoveIt 2"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"MoveIt 2"})," is the ROS 2 motion planning framework. It provides:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Motion planning"}),": RRT, RRT*, OMPL library"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Collision checking"}),": FCL (Flexible Collision Library)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Trajectory execution"}),": Smooth interpolation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"MoveGroup interface"}),": Python/C++ API"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"931-moveit-architecture",children:"9.3.1 MoveIt Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"[User Goal] \u2192 [MoveGroup] \u2192 [Planning Scene] \u2192 [OMPL Planner] \u2192 [Trajectory] \u2192 [Controller]\r\n                  \u2193\r\n            [Collision Checker]\n"})}),"\n",(0,i.jsx)(e.h3,{id:"932-installation",children:"9.3.2 Installation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# On Ubuntu 22.04\r\nsudo apt install ros-humble-moveit\r\n\r\n# Install MoveIt Python interface\r\nsudo apt install ros-humble-moveit-py\n"})}),"\n",(0,i.jsx)(e.h3,{id:"933-basic-usage",children:"9.3.3 Basic Usage"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Example"}),": Plan and execute pick-and-place"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom moveit_py import MoveItPy\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass PickAndPlace(Node):\r\n    def __init__(self):\r\n        super().__init__(\'pick_and_place\')\r\n\r\n        # Initialize MoveIt\r\n        self.moveit = MoveItPy(node_name="moveit_py")\r\n        self.arm = self.moveit.get_planning_component("manipulator")\r\n\r\n    def pick(self, object_pose):\r\n        """\r\n        Plan and execute grasp motion.\r\n\r\n        Args:\r\n            object_pose: Pose of object to grasp\r\n        """\r\n        # 1. Move to pre-grasp pose (10cm above object)\r\n        pre_grasp = object_pose.copy()\r\n        pre_grasp.pose.position.z += 0.1\r\n\r\n        self.arm.set_goal_state(pose_stamped_msg=pre_grasp)\r\n        plan1 = self.arm.plan()\r\n\r\n        if plan1.error_code.val == 1:  # Success\r\n            self.arm.execute()\r\n        else:\r\n            self.get_logger().error("Planning failed!")\r\n            return False\r\n\r\n        # 2. Open gripper\r\n        self.open_gripper()\r\n\r\n        # 3. Move down to grasp pose\r\n        self.arm.set_goal_state(pose_stamped_msg=object_pose)\r\n        plan2 = self.arm.plan()\r\n        self.arm.execute()\r\n\r\n        # 4. Close gripper\r\n        self.close_gripper()\r\n\r\n        # 5. Lift object\r\n        self.arm.set_goal_state(pose_stamped_msg=pre_grasp)\r\n        plan3 = self.arm.plan()\r\n        self.arm.execute()\r\n\r\n        return True\r\n\r\n    def place(self, place_pose):\r\n        """Similar to pick, but reverse order."""\r\n        # (Implementation similar to pick)\r\n        pass\r\n\r\n    def open_gripper(self):\r\n        # Send gripper command (hardware-specific)\r\n        pass\r\n\r\n    def close_gripper(self):\r\n        pass\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = PickAndPlace()\r\n\r\n    # Define object pose\r\n    object_pose = PoseStamped()\r\n    object_pose.header.frame_id = "base_link"\r\n    object_pose.pose.position.x = 0.5\r\n    object_pose.pose.position.y = 0.0\r\n    object_pose.pose.position.z = 0.2\r\n    object_pose.pose.orientation.w = 1.0\r\n\r\n    # Execute pick\r\n    node.pick(object_pose)\r\n\r\n    rclpy.spin(node)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"934-motion-planners",children:"9.3.4 Motion Planners"}),"\n",(0,i.jsxs)(e.p,{children:["MoveIt uses ",(0,i.jsx)(e.strong,{children:"OMPL (Open Motion Planning Library)"}),":"]}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:(0,i.jsx)(e.strong,{children:"Planner"})}),(0,i.jsx)(e.th,{children:(0,i.jsx)(e.strong,{children:"Speed"})}),(0,i.jsx)(e.th,{children:(0,i.jsx)(e.strong,{children:"Optimality"})}),(0,i.jsx)(e.th,{children:(0,i.jsx)(e.strong,{children:"Use Case"})})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"RRT"})}),(0,i.jsx)(e.td,{children:"Fast"}),(0,i.jsx)(e.td,{children:"Poor"}),(0,i.jsx)(e.td,{children:"Quick feasibility check"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsxs)(e.td,{children:[(0,i.jsx)(e.strong,{children:"RRT"}),"*"]}),(0,i.jsx)(e.td,{children:"Medium"}),(0,i.jsx)(e.td,{children:"Good"}),(0,i.jsx)(e.td,{children:"General-purpose"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"PRM"})}),(0,i.jsx)(e.td,{children:"Slow (multi-query)"}),(0,i.jsx)(e.td,{children:"Good"}),(0,i.jsx)(e.td,{children:"Repeated queries in same environment"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"STOMP"})}),(0,i.jsx)(e.td,{children:"Medium"}),(0,i.jsx)(e.td,{children:"Good"}),(0,i.jsx)(e.td,{children:"Smooth trajectories (collision-free)"})]})]})]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Configure planner"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:"# moveit_config/ompl_planning.yaml\r\nmanipulator:\r\n  planner_configs:\r\n    - RRTConnect\r\n    - RRTstar\r\n  projection_evaluator: joints(joint1,joint2)\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"94-grasp-planning",children:"9.4 Grasp Planning"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": Where to place fingers on an object?"]}),"\n",(0,i.jsx)(e.h3,{id:"941-force-closure",children:"9.4.1 Force Closure"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Definition"}),": Grasp achieves force closure if contact forces can resist ",(0,i.jsx)(e.strong,{children:"any"})," external wrench (force + torque)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"2D Example"}),": Grasping a box"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n  F\u2081 \u2502        \u2502 F\u2082\r\n  \u2190  \u2502  Box   \u2502  \u2192\r\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Force closure"}),": F\u2081 and F\u2082 point inward, opposing each other."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Not force closure"}),": F\u2081 and F\u2082 parallel (cannot resist torques)."]}),"\n",(0,i.jsx)(e.h3,{id:"942-antipodal-grasps",children:"9.4.2 Antipodal Grasps"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Parallel-jaw gripper"}),": Two fingers move in opposite directions."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Antipodal grasp"}),": Contact normals point towards each other."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),": Sample antipodal grasps for cylinder"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\n\r\ndef sample_antipodal_grasps(object_mesh, num_samples=100):\r\n    """\r\n    Sample antipodal grasps for a mesh.\r\n\r\n    Args:\r\n        object_mesh: Trimesh object\r\n        num_samples: Number of grasp candidates\r\n\r\n    Returns:\r\n        List of (position, orientation, width) tuples\r\n    """\r\n    grasps = []\r\n\r\n    for _ in range(num_samples):\r\n        # Sample random point on surface\r\n        point1, face_idx = object_mesh.sample(1, return_index=True)\r\n        normal1 = object_mesh.face_normals[face_idx[0]]\r\n\r\n        # Raycast in opposite direction\r\n        ray_origin = point1 + normal1 * 0.5\r\n        ray_direction = -normal1\r\n\r\n        locations, index_ray, index_tri = object_mesh.ray.intersects_location(\r\n            ray_origins=[ray_origin],\r\n            ray_directions=[ray_direction]\r\n        )\r\n\r\n        if len(locations) > 0:\r\n            point2 = locations[0]\r\n            normal2 = object_mesh.face_normals[index_tri[0]]\r\n\r\n            # Check if normals oppose each other\r\n            if np.dot(normal1, normal2) < -0.9:  # Nearly opposite\r\n                # Grasp center\r\n                center = (point1 + point2) / 2\r\n\r\n                # Grasp width\r\n                width = np.linalg.norm(point2 - point1)\r\n\r\n                # Grasp orientation (approach direction)\r\n                approach = normal1\r\n\r\n                grasps.append((center, approach, width))\r\n\r\n    return grasps\r\n\r\n# Example usage\r\nimport trimesh\r\nmesh = trimesh.primitives.Cylinder(radius=0.03, height=0.15)\r\ngrasps = sample_antipodal_grasps(mesh, num_samples=100)\r\n\r\nprint(f"Found {len(grasps)} grasp candidates")\r\n# Output: Found 87 grasp candidates\n'})}),"\n",(0,i.jsx)(e.h3,{id:"943-graspnet-learning-based",children:"9.4.3 GraspNet (Learning-Based)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"GraspNet-1Billion"})," (2020): DNN trained on 1 billion synthetic grasps."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Input"}),": Point cloud (from RGB-D camera)\r\n",(0,i.jsx)(e.strong,{children:"Output"}),": Ranked grasp poses (6D pose + gripper width)"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\r\nfrom graspnetAPI import GraspNet\r\n\r\n# Load pre-trained model\r\nmodel = GraspNet(checkpoint_path=\'graspnet_checkpoint.pth\')\r\n\r\n# Capture point cloud from RealSense\r\npoint_cloud = capture_pointcloud()  # Nx3 array\r\n\r\n# Predict grasps\r\ngrasps = model.infer(point_cloud)\r\n\r\n# Sort by score\r\ngrasps_sorted = sorted(grasps, key=lambda g: g.score, reverse=True)\r\n\r\n# Best grasp\r\nbest_grasp = grasps_sorted[0]\r\nprint(f"Grasp pose: {best_grasp.pose}")\r\nprint(f"Gripper width: {best_grasp.width:.3f}m")\r\nprint(f"Score: {best_grasp.score:.2f}")\r\n\r\n# Execute with MoveIt\r\nexecute_grasp(best_grasp)\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"95-force-control",children:"9.5 Force Control"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": Position control is rigid (robot ignores contact forces)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Use cases"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Insertion"}),": Inserting USB plug (requires compliance)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Wiping"}),": Cleaning table (constant force)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Assembly"}),": Peg-in-hole (feel for contact)"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"951-impedance-control",children:"9.5.1 Impedance Control"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Act like a spring-damper system."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Control law"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"F = K(x_desired - x) + B(\u1e8b_desired - \u1e8b)\n"})}),"\n",(0,i.jsx)(e.p,{children:"Where:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"K"}),": Stiffness (N/m)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"B"}),": Damping (Ns/m)"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def impedance_controller(x_desired, x_current, v_current, K=100, B=10):\r\n    """\r\n    Compute force command for impedance control.\r\n\r\n    Args:\r\n        x_desired: Desired position (m)\r\n        x_current: Current position (m)\r\n        v_current: Current velocity (m/s)\r\n        K: Stiffness (N/m)\r\n        B: Damping (Ns/m)\r\n\r\n    Returns:\r\n        F: Force command (N)\r\n    """\r\n    error = x_desired - x_current\r\n    F = K * error - B * v_current\r\n    return F\r\n\r\n# Example: Move to target with compliance\r\nx_target = 0.5\r\nx = 0.0\r\nv = 0.0\r\ndt = 0.01\r\nm = 1.0  # End-effector mass (kg)\r\n\r\nfor t in np.arange(0, 2, dt):\r\n    F = impedance_controller(x_target, x, v, K=100, B=10)\r\n\r\n    # Simulate dynamics\r\n    a = F / m\r\n    v += a * dt\r\n    x += v * dt\r\n\r\n    print(f"t={t:.2f}, x={x:.3f}, F={F:.2f}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"952-admittance-control",children:"9.5.2 Admittance Control"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Opposite of impedance"}),": Sense force \u2192 produce motion."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Control law"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u1e8b = (F_measured - F_desired) / B\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Use case"}),": Sanding (apply constant force to surface)."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"96-visual-servoing",children:"9.6 Visual Servoing"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Visual servoing"}),": Use camera feedback to guide robot motion."]}),"\n",(0,i.jsx)(e.h3,{id:"961-position-based-visual-servoing-pbvs",children:"9.6.1 Position-Based Visual Servoing (PBVS)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Workflow"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Detect object in camera image"}),"\n",(0,i.jsx)(e.li,{children:"Estimate object pose (using DOPE, FoundationPose)"}),"\n",(0,i.jsx)(e.li,{children:"Compute end-effector target pose"}),"\n",(0,i.jsx)(e.li,{children:"Move to target using MoveIt"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def pbvs_grasp(camera, detector, arm):\r\n    """\r\n    Grasp object using position-based visual servoing.\r\n    """\r\n    # 1. Capture image\r\n    image = camera.capture()\r\n\r\n    # 2. Detect object\r\n    object_pose = detector.detect(image)  # Returns 6D pose\r\n\r\n    if object_pose is None:\r\n        print("Object not detected!")\r\n        return False\r\n\r\n    # 3. Compute grasp pose (offset from object)\r\n    grasp_pose = object_pose.copy()\r\n    grasp_pose.position.z += 0.15  # Approach from above\r\n\r\n    # 4. Plan and execute\r\n    arm.set_goal_state(pose_stamped_msg=grasp_pose)\r\n    plan = arm.plan()\r\n\r\n    if plan.error_code.val == 1:\r\n        arm.execute()\r\n        return True\r\n    else:\r\n        return False\n'})}),"\n",(0,i.jsx)(e.h3,{id:"962-image-based-visual-servoing-ibvs",children:"9.6.2 Image-Based Visual Servoing (IBVS)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Workflow"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Track features in image (e.g., corners)"}),"\n",(0,i.jsx)(e.li,{children:"Compute image Jacobian (maps joint velocities to feature velocities)"}),"\n",(0,i.jsx)(e.li,{children:"Move features towards desired positions"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advantage"}),": No need for 3D pose estimation (works even with calibration errors)."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"97-dual-arm-manipulation",children:"9.7 Dual-Arm Manipulation"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Use cases"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Carry large objects"}),": Two arms share load"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Bi-manual tasks"}),": Open jar (one hand holds, one unscrews)"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"971-coordination",children:"9.7.1 Coordination"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Challenge"}),": Synchronize two arms (avoid internal forces)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Approach"}),": Plan for ",(0,i.jsx)(e.strong,{children:"relative pose"})," (maintain grasp while moving)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def dual_arm_carry(left_arm, right_arm, object_pose, target_pose):\r\n    """\r\n    Carry object with two arms.\r\n\r\n    Args:\r\n        left_arm, right_arm: MoveIt arm interfaces\r\n        object_pose: Current object pose\r\n        target_pose: Desired object pose\r\n    """\r\n    # Current grasp poses\r\n    left_grasp = compute_grasp_pose(object_pose, side=\'left\')\r\n    right_grasp = compute_grasp_pose(object_pose, side=\'right\')\r\n\r\n    # Target grasp poses (maintain relative transform)\r\n    left_target = compute_grasp_pose(target_pose, side=\'left\')\r\n    right_target = compute_grasp_pose(target_pose, side=\'right\')\r\n\r\n    # Plan for both arms simultaneously\r\n    multi_arm_planner = MoveGroupInterface("arms")\r\n    multi_arm_planner.set_pose_target(left_target, end_effector_link="left_gripper")\r\n    multi_arm_planner.set_pose_target(right_target, end_effector_link="right_gripper")\r\n\r\n    plan = multi_arm_planner.plan()\r\n    multi_arm_planner.execute(plan)\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"98-whole-body-manipulation-humanoids",children:"9.8 Whole-Body Manipulation (Humanoids)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advantage"}),": Use base + torso + arms (12+ DOF total)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Use case"}),": Reach high shelf (stand on toes + extend arm)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Example"}),": Whole-body IK"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import pinocchio as pin\r\n\r\ndef whole_body_ik(model, data, hand_target, foot_contacts):\r\n    """\r\n    IK for humanoid (optimizes all joints).\r\n\r\n    Args:\r\n        model: Pinocchio model\r\n        data: Pinocchio data\r\n        hand_target: Desired hand pose\r\n        foot_contacts: List of foot frame IDs (must stay on ground)\r\n\r\n    Returns:\r\n        q: Joint configuration\r\n    """\r\n    q = pin.neutral(model)\r\n\r\n    for iteration in range(100):\r\n        pin.forwardKinematics(model, data, q)\r\n        pin.computeJointJacobians(model, data, q)\r\n\r\n        # Hand task (priority 1)\r\n        hand_id = model.getFrameId("left_hand")\r\n        J_hand = pin.getFrameJacobian(model, data, hand_id, pin.LOCAL_WORLD_ALIGNED)\r\n\r\n        current_pose = data.oMf[hand_id]\r\n        error_hand = hand_target.translation - current_pose.translation\r\n\r\n        # Foot tasks (priority 2: stay on ground)\r\n        errors = [error_hand]\r\n        jacobians = [J_hand[:3, :]]  # Position only\r\n\r\n        for foot_id in foot_contacts:\r\n            J_foot = pin.getFrameJacobian(model, data, foot_id, pin.LOCAL_WORLD_ALIGNED)\r\n            foot_pos = data.oMf[foot_id].translation\r\n            error_foot = np.array([0, 0, 0]) - foot_pos  # Stay at current position\r\n            errors.append(error_foot)\r\n            jacobians.append(J_foot[:3, :])\r\n\r\n        # Stack constraints\r\n        J = np.vstack(jacobians)\r\n        e = np.concatenate(errors)\r\n\r\n        # Solve\r\n        dq = np.linalg.pinv(J) @ e\r\n        q = pin.integrate(model, q, dq * 0.1)\r\n\r\n        if np.linalg.norm(e) < 1e-3:\r\n            break\r\n\r\n    return q\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-91-compare-ik-solvers",children:"Exercise 9.1: Compare IK Solvers"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Implement and benchmark 5 IK solvers:"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Jacobian pseudoinverse"}),"\n",(0,i.jsx)(e.li,{children:"Damped least squares"}),"\n",(0,i.jsx)(e.li,{children:"PyKDL"}),"\n",(0,i.jsx)(e.li,{children:"MoveIt (IKFast)"}),"\n",(0,i.jsx)(e.li,{children:"TracIK"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Metrics"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Speed"}),": Average solve time (ms)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Success rate"}),": % of reachable targets solved"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Accuracy"}),": Mean pose error (mm)"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected result"}),": TracIK wins on success rate; IKFast wins on speed."]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-92-stack-3-blocks",children:"Exercise 9.2: Stack 3 Blocks"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Use MoveIt to stack 3 blocks (pick-and-place)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Blocks start at random positions on table"}),"\n",(0,i.jsx)(e.li,{children:"Stack them in order: Red \u2192 Blue \u2192 Green"}),"\n",(0,i.jsx)(e.li,{children:"Measure success rate over 20 trials"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected"}),": >85% success with tuned grasp poses."]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-93-train-graspnet-on-ycb-objects",children:"Exercise 9.3: Train GraspNet on YCB Objects"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Goal"}),": Fine-tune GraspNet on YCB dataset (77 household objects)."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Generate synthetic grasps in Isaac Sim (10,000 per object)"}),"\n",(0,i.jsx)(e.li,{children:"Train GraspNet model (PyTorch)"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate on 10 novel objects"}),"\n",(0,i.jsx)(e.li,{children:"Compare to baseline (random grasps)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected"}),": GraspNet achieves >80% success vs 40% for random."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"citations",children:"Citations"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Salisbury, J. K., & Roth, B. (1983). ",(0,i.jsx)(e.em,{children:"Kinematic and force analysis of articulated mechanical hands."})," Journal of Mechanisms, Transmissions, and Automation in Design, 105(1), 35-41."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Chitta, S., Sucan, I., & Cousins, S. (2012). ",(0,i.jsx)(e.em,{children:"MoveIt! [ROS Topics]."})," IEEE Robotics & Automation Magazine, 19(1), 18-19."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Kuffner, J. J., & LaValle, S. M. (2000). ",(0,i.jsx)(e.em,{children:"RRT-connect: An efficient approach to single-query path planning."})," IEEE International Conference on Robotics and Automation (ICRA), 2, 995-1001."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Fang, H. S., Wang, C., Gou, M., & Lu, C. (2020). ",(0,i.jsx)(e.em,{children:"GraspNet-1Billion: A large-scale benchmark for general object grasping."})," IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 11444-11453."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Hogan, N. (1985). ",(0,i.jsx)(e.em,{children:"Impedance control: An approach to manipulation: Part I\u2014Theory."})," Journal of Dynamic Systems, Measurement, and Control, 107(1), 1-7."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Hutchinson, S., Hager, G. D., & Corke, P. I. (1996). ",(0,i.jsx)(e.em,{children:"A tutorial on visual servo control."})," IEEE Transactions on Robotics and Automation, 12(5), 651-670."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Beeson, P., & Ames, B. (2015). ",(0,i.jsx)(e.em,{children:"TRAC-IK: An open-source library for improved solving of generic inverse kinematics."})," IEEE-RAS International Conference on Humanoid Robots (Humanoids), 928-935."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Sucan, I. A., & Chitta, S. (2013). ",(0,i.jsx)(e.em,{children:"MoveIt! Motion planning framework."})," ",(0,i.jsx)(e.a,{href:"http://moveit.ros.org",children:"http://moveit.ros.org"})]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Miller, A. T., & Allen, P. K. (2004). ",(0,i.jsx)(e.em,{children:"GraspIt! A versatile simulator for robotic grasping."})," IEEE Robotics & Automation Magazine, 11(4), 110-122."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Mahler, J., Liang, J., Niyaz, S., Laskey, M., Doan, R., Liu, X., ... & Goldberg, K. (2017). ",(0,i.jsx)(e.em,{children:"Dex-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics."})," Robotics: Science and Systems (RSS)."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Carpentier, J., & Mansard, N. (2018). ",(0,i.jsx)(e.em,{children:"Analytical derivatives of rigid body dynamics algorithms."})," Robotics: Science and Systems (RSS)."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:["Kalakrishnan, M., Chitta, S., Theodorou, E., Pastor, P., & Schaal, S. (2011). ",(0,i.jsx)(e.em,{children:"STOMP: Stochastic trajectory optimization for motion planning."})," IEEE International Conference on Robotics and Automation (ICRA), 4569-4574."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Simulation Only"})," (recommended):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"CPU/GPU"}),": Any modern computer"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Software"}),": MoveIt 2 + Gazebo/RViz"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Real Hardware"})," (optional):"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Budget"}),": Unitree Z1 arm ($3,000)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Research"}),": Franka Emika Panda ($25,000)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Humanoid"}),": Unitree G1 ($16,000)"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"labs",children:"Labs"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Lab 9.1: MoveIt Pick-and-Place"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Repository"}),": ",(0,i.jsx)(e.a,{href:"https://github.com/Shumailaaijaz/physical-ai-labs",children:"github.com/Shumailaaijaz/physical-ai-labs"})]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Lab Path"}),": ",(0,i.jsx)(e.code,{children:"labs/chapter-09-moveit-planning/"})]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"What's Included"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"MoveIt config for Panda arm"}),"\n",(0,i.jsx)(e.li,{children:"Gazebo world with table + blocks"}),"\n",(0,i.jsx)(e.li,{children:"Pick-and-place Python script"}),"\n",(0,i.jsx)(e.li,{children:"RViz visualization"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected Time"}),": 2 hours"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Lab 9.2: GraspNet Inference"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Lab Path"}),": ",(0,i.jsx)(e.code,{children:"labs/chapter-09-graspnet/"})]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"What's Included"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Pre-trained GraspNet checkpoint"}),"\n",(0,i.jsx)(e.li,{children:"RealSense D435i point cloud capture"}),"\n",(0,i.jsx)(e.li,{children:"Grasp visualization"}),"\n",(0,i.jsx)(e.li,{children:"MoveIt execution integration"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Hardware"}),": RealSense D435i (or use pre-recorded point clouds)"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Expected Time"}),": 3 hours"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/10-vision-language-action",children:"Chapter 10: Vision-Language-Action Models \u2192"})]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsxs)(e.em,{children:["This textbook is a living document. Found an error? Have a suggestion? Submit an issue or PR at ",(0,i.jsx)(e.a,{href:"https://github.com/Shumailaaijaz/physical-ai-textbook",children:"github.com/Shumailaaijaz/physical-ai-textbook"})]})})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>t,x:()=>a});var s=r(6540);const i={},o=s.createContext(i);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);