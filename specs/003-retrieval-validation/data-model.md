# Data Model: Retrieval & Pipeline Validation

**Feature**: Retrieval & Pipeline Validation
**Date**: 2025-12-25
**Status**: Final

## Overview

This document defines the data structures used by `retrieve.py` for validation workflow. These are ephemeral runtime objects (not persisted to database), used for organizing validation logic and report generation.

---

## Entities

### 1. QueryRequest

**Purpose**: Encapsulates user's validation query and search parameters

**Source**: CLI arguments parsed by argparse

**Attributes**:

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `query_text` | `str` | Yes | "What are vector databases?" | User's search query |
| `top_k` | `int` | Yes | 5 | Number of results to retrieve (1-20) |
| `min_score` | `float` | Yes | 0.0 | Minimum similarity score threshold (0.0-1.0) |

**Validation Rules**:
- `query_text`: Non-empty string, max 2000 characters
- `top_k`: Integer in range [1, 20]
- `min_score`: Float in range [0.0, 1.0]

**Example**:
```python
request = QueryRequest(
    query_text="How does reinforcement learning work?",
    top_k=10,
    min_score=0.3
)
```

**Lifecycle**: Created at script start, used throughout validation workflow

---

### 2. QueryEmbedding

**Purpose**: Vector representation of the query, generated by Cohere

**Source**: Cohere API response (`embed()` method)

**Attributes**:

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `vector` | `list[float]` | Yes | - | 1024-dimensional embedding |
| `model` | `str` | Yes | - | Embedding model name (embed-multilingual-v3.0) |
| `input_type` | `str` | Yes | "search_query" | Cohere input type parameter |

**Validation Rules**:
- `vector`: Must have exactly 1024 elements
- `model`: Must be "embed-multilingual-v3.0"
- `input_type`: Must be "search_query" (not "search_document")

**Example**:
```python
embedding = QueryEmbedding(
    vector=[0.123, -0.456, 0.789, ...],  # 1024 floats
    model="embed-multilingual-v3.0",
    input_type="search_query"
)
```

**Lifecycle**: Created after Cohere API call, passed to Qdrant search

---

### 3. RetrievedChunk

**Purpose**: Represents a single search result from Qdrant with metadata

**Source**: Qdrant `query_points()` response

**Attributes**:

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `chunk_id` | `str` | Yes | - | UUID of the chunk |
| `text` | `str` | Yes | - | Chunk content (plain text) |
| `similarity_score` | `float` | Yes | - | Cosine similarity (0.0-1.0) |
| `source_url` | `str` | Yes | - | Origin URL from textbook |
| `chapter` | `str` | No | None | Chapter title |
| `section` | `str` | No | None | Section heading |
| `chunk_index` | `int` | No | None | Position in source document |
| `token_count` | `int` | No | None | Number of tokens in chunk |

**Validation Rules**:
- `chunk_id`: Valid UUID format
- `text`: Non-empty string
- `similarity_score`: Float in range [0.0, 1.0]
- `source_url`: Must start with `https://shumailaaijaz.github.io/physical-ai-textbook/`
- `chapter`, `section`: Optional, may be None/empty
- `chunk_index`, `token_count`: Optional, may be None

**Example**:
```python
chunk = RetrievedChunk(
    chunk_id="550e8400-e29b-41d4-a716-446655440000",
    text="Vector databases store high-dimensional embeddings...",
    similarity_score=0.523,
    source_url="https://shumailaaijaz.github.io/physical-ai-textbook/docs/10-vision-language-action",
    chapter="Chapter 10: Vision-Language-Action (VLA) Models",
    section="Vector Database Fundamentals",
    chunk_index=3,
    token_count=512
)
```

**Lifecycle**: Created for each search result, validated, displayed in report

---

### 4. ValidationCheck

**Purpose**: Result of a single validation test with status and details

**Source**: Generated by validation functions (`validate_connection()`, `validate_results()`, etc.)

**Attributes**:

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `check_name` | `str` | Yes | - | Name of validation (e.g., "Metadata Completeness") |
| `status` | `Literal['PASS', 'FAIL', 'WARNING']` | Yes | - | Check result |
| `message` | `str` | Yes | - | Explanation or error details |
| `affected_items` | `list[str]` | No | [] | IDs of failed items (e.g., chunk IDs) |

**Status Values**:
- `PASS`: Check succeeded, no issues detected
- `FAIL`: Critical failure, script should exit with code 1
- `WARNING`: Non-critical issue, script exits with code 2

**Example**:
```python
check_pass = ValidationCheck(
    check_name="Connection",
    status="PASS",
    message="Connected to Qdrant, collection has 301 points",
    affected_items=[]
)

check_fail = ValidationCheck(
    check_name="Metadata Completeness",
    status="FAIL",
    message="2 chunks missing required field 'source_url'",
    affected_items=["abc123", "def456"]
)

check_warning = ValidationCheck(
    check_name="Result Count",
    status="WARNING",
    message="No results found - query may be outside textbook scope",
    affected_items=[]
)
```

**Lifecycle**: Created during validation, accumulated in list, printed in report

---

### 5. ValidationReport

**Purpose**: Aggregated results of all validation checks with overall status

**Source**: Generated at end of `main()` after all checks complete

**Attributes**:

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `query` | `str` | Yes | - | Query that was tested |
| `results_count` | `int` | Yes | - | Number of chunks retrieved |
| `checks` | `list[ValidationCheck]` | Yes | - | Individual validation results |
| `overall_status` | `Literal['PASS', 'FAIL', 'WARNING']` | Yes | - | Aggregated status |
| `top_results` | `list[RetrievedChunk]` | Yes | - | Search results to display |

**Aggregation Logic**:
- `overall_status = 'FAIL'` if any check has status='FAIL'
- `overall_status = 'WARNING'` if no FAIL but any check has status='WARNING'
- `overall_status = 'PASS'` if all checks have status='PASS'

**Example**:
```python
report = ValidationReport(
    query="What are vector databases?",
    results_count=5,
    checks=[
        ValidationCheck("Connection", "PASS", "Connected successfully"),
        ValidationCheck("Collection Config", "PASS", "1024-dim, cosine"),
        ValidationCheck("Query Embedding", "PASS", "1024 dimensions"),
        ValidationCheck("Search Execution", "PASS", "5 results"),
        ValidationCheck("Metadata Completeness", "PASS", "5/5 chunks complete"),
        ValidationCheck("Score Validation", "PASS", "Scores in range, descending")
    ],
    overall_status="PASS",
    top_results=[chunk1, chunk2, chunk3, chunk4, chunk5]
)
```

**Lifecycle**: Created at end of validation workflow, passed to `print_report()`

---

## Data Flow

```
CLI Arguments
     ↓
QueryRequest (query_text, top_k, min_score)
     ↓
Cohere API
     ↓
QueryEmbedding (vector, model, input_type)
     ↓
Qdrant API
     ↓
list[RetrievedChunk] (chunk_id, text, score, metadata)
     ↓
Validation Functions
     ↓
list[ValidationCheck] (check_name, status, message)
     ↓
ValidationReport (query, results, checks, overall_status)
     ↓
print_report() → stdout/stderr
     ↓
Exit Code (0=PASS, 1=FAIL, 2=WARNING)
```

---

## Validation Checks Defined

### Check 1: Connection
**Function**: `validate_connection()`
**Purpose**: Verify Qdrant connection and collection existence
**PASS Criteria**: Successfully connected, collection 'rag_embeddings' exists
**FAIL Criteria**: Connection error, missing collection
**Affected Items**: N/A

### Check 2: Collection Config
**Function**: `validate_collection_config()`
**Purpose**: Verify collection schema matches expected configuration
**PASS Criteria**: Vector size=1024, distance=cosine
**FAIL Criteria**: Dimension mismatch, wrong distance metric
**Affected Items**: N/A

### Check 3: Query Embedding
**Function**: `generate_query_embedding()`
**Purpose**: Generate valid query embedding
**PASS Criteria**: Embedding generated, 1024 dimensions
**FAIL Criteria**: Cohere API error, dimension mismatch
**Affected Items**: N/A

### Check 4: Search Execution
**Function**: `perform_search()`
**Purpose**: Execute vector search successfully
**PASS Criteria**: Search completes, results returned
**WARNING Criteria**: 0 results (query outside scope)
**FAIL Criteria**: Qdrant error, timeout
**Affected Items**: N/A

### Check 5: Metadata Completeness
**Function**: `validate_results()` (metadata validation)
**Purpose**: Verify all chunks have required metadata
**PASS Criteria**: All chunks have `text` and `source_url`
**WARNING Criteria**: Missing optional fields (`chapter`, `section`)
**FAIL Criteria**: Missing required fields (`text`, `source_url`)
**Affected Items**: List of chunk IDs with missing metadata

### Check 6: Score Validation
**Function**: `validate_results()` (score validation)
**Purpose**: Verify similarity scores are valid and ordered
**PASS Criteria**: All scores in [0.0, 1.0], descending order
**FAIL Criteria**: Scores outside range, incorrect ordering
**Affected Items**: List of chunk IDs with invalid scores

---

## Implementation Notes

### Python Representation

Since this is a simple validation script, we'll use dictionaries/named tuples instead of full Pydantic models:

```python
from typing import Literal, NamedTuple

class QueryRequest(NamedTuple):
    query_text: str
    top_k: int
    min_score: float

class QueryEmbedding(NamedTuple):
    vector: list[float]
    model: str
    input_type: str

class RetrievedChunk(NamedTuple):
    chunk_id: str
    text: str
    similarity_score: float
    source_url: str
    chapter: str | None
    section: str | None
    chunk_index: int | None
    token_count: int | None

class ValidationCheck(NamedTuple):
    check_name: str
    status: Literal['PASS', 'FAIL', 'WARNING']
    message: str
    affected_items: list[str] = []

class ValidationReport(NamedTuple):
    query: str
    results_count: int
    checks: list[ValidationCheck]
    overall_status: Literal['PASS', 'FAIL', 'WARNING']
    top_results: list[RetrievedChunk]
```

**Rationale**: NamedTuples provide type safety without Pydantic dependency, sufficient for ephemeral validation data.

---

## Storage

**None of these entities are persisted**. All data is ephemeral runtime state:
- `QueryRequest`: Parsed from CLI arguments
- `QueryEmbedding`: Returned from Cohere API
- `RetrievedChunk`: Returned from Qdrant API
- `ValidationCheck`: Generated during validation
- `ValidationReport`: Aggregated at end, printed to stdout/stderr

**Existing Qdrant Collection Schema** (from Phase 0, read-only):
- Collection: `rag_embeddings`
- Vector size: 1024 dimensions
- Distance metric: Cosine
- Payload fields: `text`, `source_url`, `chapter`, `section`, `chunk_index`, `char_start`, `char_end`, `token_count`, `embedding_model`, `ingestion_timestamp`

---

## Exit Code Mapping

| Overall Status | Exit Code | Description |
|----------------|-----------|-------------|
| `PASS` | 0 | All validation checks passed |
| `FAIL` | 1 | One or more critical checks failed |
| `WARNING` | 2 | Non-critical issues detected (empty results, missing optional metadata) |

---

## Summary

This data model defines 5 key entities for validation workflow:
1. **QueryRequest**: User input (query, top-k, min-score)
2. **QueryEmbedding**: Cohere API response (vector, model, input_type)
3. **RetrievedChunk**: Qdrant search result (chunk_id, text, score, metadata)
4. **ValidationCheck**: Individual check result (check_name, status, message)
5. **ValidationReport**: Aggregated report (query, results, checks, overall_status)

All entities are ephemeral runtime objects (not persisted). Implementation uses NamedTuples for type safety without external dependencies.
